<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><title>letnet | 黎思敏的个人学习成长纪录</title><meta name="author" content="黎思敏"><meta name="copyright" content="黎思敏"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#f7f9fe"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-touch-fullscreen" content="yes"><meta name="apple-mobile-web-app-title" content="letnet"><meta name="application-name" content="letnet"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="#f7f9fe"><meta property="og:type" content="article"><meta property="og:title" content="letnet"><meta property="og:url" content="http://example.com/2025/10/07/7.ViT/index.html"><meta property="og:site_name" content="黎思敏的个人学习成长纪录"><meta property="og:description" content="7.VITVision Transformer。这篇论文首次证明，纯Transformer架构无需卷积网络的辅助，直接在图像分类任务上就能达到顶尖水平，打破了计算机视觉长期被卷积神经网络统治的局面。 论文核心信息 标题: An Image is Worth 16x16 Words: Transfor"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta property="article:author" content="黎思敏"><meta property="article:tag"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg"><meta name="description" content="7.VITVision Transformer。这篇论文首次证明，纯Transformer架构无需卷积网络的辅助，直接在图像分类任务上就能达到顶尖水平，打破了计算机视觉长期被卷积神经网络统治的局面。 论文核心信息 标题: An Image is Worth 16x16 Words: Transfor"><link rel="shortcut icon" href="/favicon.ico"><link rel="canonical" href="http://example.com/2025/10/07/7.ViT/"><link rel="preconnect" href="//cdn.cbd.int"/><meta name="google-site-verification" content="xxx"/><meta name="baidu-site-verification" content="code-xxx"/><meta name="msvalidate.01" content="xxx"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  linkPageTop: undefined,
  peoplecanvas: {"enable":true,"img":"https://upload-bbs.miyoushe.com/upload/2024/07/27/125766904/ba62475f396df9de3316a08ed9e65d86_5680958632268053399..png"},
  postHeadAiDescription: {"enable":true,"gptName":"AnZhiYu","mode":"local","switchBtn":false,"btnLink":"https://afdian.net/item/886a79d4db6711eda42a52540025c377","randomNum":3,"basicWordCount":1000,"key":"xxxx","Referer":"https://xx.xx/"},
  diytitle: {"enable":true,"leaveTitle":"w(ﾟДﾟ)w 不要走！再看看嘛！","backTitle":"♪(^∇^*)欢迎肥来！"},
  LA51: undefined,
  greetingBox: {"enable":true,"default":"晚上好👋","list":[{"greeting":"晚安😴","startTime":0,"endTime":5},{"greeting":"早上好鸭👋, 祝你一天好心情！","startTime":6,"endTime":9},{"greeting":"上午好👋, 状态很好，鼓励一下～","startTime":10,"endTime":10},{"greeting":"11点多啦, 在坚持一下就吃饭啦～","startTime":11,"endTime":11},{"greeting":"午安👋, 宝贝","startTime":12,"endTime":14},{"greeting":"🌈充实的一天辛苦啦！","startTime":14,"endTime":18},{"greeting":"19点喽, 奖励一顿丰盛的大餐吧🍔。","startTime":19,"endTime":19},{"greeting":"晚上好👋, 在属于自己的时间好好放松😌~","startTime":20,"endTime":24}]},
  twikooEnvId: '',
  commentBarrageConfig:undefined,
  music_page_default: "nav_music",
  root: '/',
  preloader: {"source":3},
  friends_vue_info: undefined,
  navMusic: true,
  mainTone: undefined,
  authorStatus: {"skills":["🤖️ 数码科技爱好者","🔍 分享与热心帮助","🏠 智能家居小能手","🔨 设计开发一条龙","🤝 专修交互与设计","🏃 脚踏实地行动派","🧱 团队小组发动机","💢 壮汉人狠话不多"]},
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简","rightMenuMsgToTraditionalChinese":"转为繁体","rightMenuMsgToSimplifiedChinese":"转为简体"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":330},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    simplehomepage: true,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"copy":true,"copyrightEbable":false,"limitCount":50,"languages":{"author":"作者: 黎思敏","link":"链接: ","source":"来源: 黎思敏的个人学习成长纪录","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","copySuccess":"复制成功，复制和转载请标注本文地址"}},
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#425AEF","bgDark":"#1f1f1f","position":"top-center"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.min.js',
      css: 'https://cdn.cbd.int/flickr-justified-gallery@2.1.2/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  shortcutKey: undefined,
  autoDarkmode: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  configTitle: '黎思敏的个人学习成长纪录',
  title: 'letnet',
  postAI: '',
  pageFillDescription: '7.VIT, 论文核心信息, 一、背景与动机：为什么要在视觉中用Transformer？, 二、模型架构：如何将图像转化为序列？, 1. 图像分块嵌入, 2. [class]令牌与位置编码, 3. Transformer编码器, 三、核心创新与关键发现, 1. 摒弃卷积归纳偏置, 2. 尺度定律：数据量是关键, 3. 自注意力的全局视野, 四、实验结果与影响, 1. 主要结果, 2. 历史意义, 五、ViT的局限性, 总结, 卷积神经网络（Convolutional Neural Network CNN） 进行一次全面而详细的介绍。, 一、 核心思想与两大优势, 二、 CNN的核心组成模块, 1. 卷积层 - 特征提取的核心, 2. 池化层 - 降维与保持平移不变性, 3. 全连接层 - 最终分类x2F回归, 三、 CNN的经典工作流程, 四、 经典CNN架构, 五、 主要应用领域, 总结, 循环神经网络（Recurrent Neural Network RNN） 进行一次全面而详细的介绍。, 一、 核心思想：处理序列与引入记忆, 二、 RNN的核心工作原理与结构, 1. 循环单元：展开视角, 2. 不同的输入输出模式, 三、 RNN的挑战与进阶变体, 1. 梯度消失与梯度爆炸问题, 2. 长短期记忆网络（LSTM）, 3. 门控循环单元（GRU）, 四、 RNN的典型应用领域, 五、 RNN的现状与未来, 总结这篇论文首次证明纯架构无需卷积网络的辅助直接在图像分类任务上就能达到顶尖水平打破了计算机视觉长期被卷积神经网络统治的局面论文核心信息标题作者会议核心思想将图像分割成固定大小的图像块将其线性嵌入后作为序列输入到标准的编码器中进行图像分类一背景与动机为什么要在视觉中用在之前计算机视觉几乎是卷积神经网络的天下从到通过其局部连接和平移不变性等归纳偏置在图像任务上取得了巨大成功与此同时在自然语言处理领域凭借其自注意力机制和全局建模能力完全取代了成为绝对的主流一个很自然的问题是这种强大的架构能否在计算机视觉领域同样取得成功的雄心探索完全摒弃卷积操作仅使用标准的架构来处理图像并测试其性能极限二模型架构如何将图像转化为序列是为序列数据如单词序列设计的而图像是二维结构最核心最创新的部分就在于它如何将图像转换为可以处理的序列图像分块嵌入这是模型的起点其处理流程如下图所示输入图像分割为图像块得到个块展平与线性投影每个块变为维向量添加可学习的令牌添加可学习的位置编码编码器的输入序列图像分块将输入图像例如分割成多个固定大小的图像块例如这样图像就被划分为个图像块图像块展平与线性投影将每个图像块维展平为一个向量然后通过一个可训练的线性投影层全连接层映射到模型指定的维度例如这个投影的输出被称为图像块嵌入其作用类似于中的词嵌入这正是论文标题一张图像等价于个单词的由来模型将每个的图像块视为一个单词令牌与位置编码令牌在图像块嵌入序列的前面预先添加一个可学习的嵌入向量这个令牌的作用类似于中的令牌在经过编码器处理后其对应的输出状态将作为整个图像的全局表示用于最终的分类位置编码由于本身是置换不变的它无法感知序列中元素的顺序但图像块的空间位置信息至关重要因此为每个图像块嵌入添加一个可学习的一维位置编码以保留图像块在原始图像中的位置信息编码器将处理好的序列令牌图像块嵌入位置编码输入到一个标准的编码器中编码器由个相同的层堆叠而成每层包含多头自注意力机制使每个图像块包括令牌都能够与所有其他图像块进行交互从而捕获全局的上下文信息多层感知机对每个位置的特征进行非线性变换层归一化和残差连接用于稳定和加速训练最终将令牌对应的输出向量输入一个分类头通常是一个简单的线性层得到最终的分类结果三核心创新与关键发现摒弃卷积归纳偏置的成功很大程度上归功于其内置的归纳偏置例如局部性相邻像素关联性强和平移不变性几乎没有任何针对图像的强归纳偏置它只在最初的分块阶段隐含了局部性在层中是平移等变的但自注意力层本身是全局的置换敏感的这意味着必须完全从数据中学习所有关于图像空间结构的知识尺度定律数据量是关键论文最关键的发现是的性能强烈依赖于训练数据集的规模在中型数据集如上训练的表现不如同等规模的模型如因为它缺乏的归纳偏置需要更多数据来学习这些视觉概念在超大规模数据集如包含亿张图像上预训练的表现显著超越了最先进的模型这表明当数据足够多时强大的表示学习能力可以完全弥补其缺乏归纳偏置的弱点甚至做得更好自注意力的全局视野与的局部感受野不同从第一层开始就拥有全局感受野每个图像块都可以直接与任何其他图像块交互这使得能够更早更有效地建模图像中长距离的依赖关系四实验结果与影响主要结果分类在上最大的模型达到了的准确率超越了当时最好的模型计算效率在预训练时展现出优异的计算效率在达到相似性能时所需的预训练计算量远少于历史意义打破了的垄断证明了纯架构在视觉任务上的巨大潜力开辟了视觉研究的新方向推动了视觉与语言的统一由于和可以使用相同的骨干网络为后来的多模态模型如奠定了坚实的基础催生了一系列变体如引入层次结构和局部性无需大规模预训练掩码自编码器等形成了庞大的家族五的局限性数据饥渴严重依赖大规模预训练数据对于没有海量数据的研究者不友好计算复杂度高自注意力的计算复杂度与序列长度的平方成正比处理高分辨率图像时图像块数量剧增导致计算开销巨大缺乏空间层次结构原始始终保持单一尺度的特征图对需要多尺度特征的密集预测任务如目标检测分割不友好总结论文的核心贡献在于它进行了一次大胆而成功的探索将中极其成功的标准架构几乎不做修改地应用到了图像领域它回答了什么问题能否成为通用的视觉骨干网络答案是可以但前提是需要足够规模的数据它的解决方案是什么将图像拆分为块序列通过线性嵌入和位置编码将其转化为的输入格式它带来了什么影响它打破了卷积是视觉必备的思维定式引领了视觉的研究浪潮并为构建统一的视觉语言模型架构铺平了道路是计算机视觉领域一个真正的范式转变卷积神经网络进行一次全面而详细的介绍是一种专门设计用于处理具有网格状拓扑结构数据如图像视频音频频谱图的深度学习模型它在计算机视觉领域取得了革命性的成功是当今图像识别目标检测图像分割等技术的核心基础一核心思想与两大优势传统的全连接神经网络在处理图像时会将三维高度宽度通道的图像展平为一维向量这会导致两个严重问题参数爆炸一张像素的图片展平后输入层就有万个节点如果第一个隐藏层也有万个节点那么仅这一层的参数就高达万亿个这极大地增加了计算负担和过拟合风险忽略空间结构展平操作破坏了像素在空间上的邻近关系而图像中的物体正是由这些在空间上相邻的像素构成的通过两种核心思想解决了以上问题局部感受野每个神经元只与前一层的一个小区域局部区域相连而不是与所有神经元全连接这模拟了生物视觉系统中视网膜细胞只感受特定区域刺激的特性参数共享同一个特征检测器卷积核在输入数据的整个空间范围内滑动重复使用相同的参数这极大地减少了模型参数这两大思想具体通过卷积操作来实现二的核心组成模块一个典型的由一系列层堆叠而成主要包括以下三种核心层卷积层特征提取的核心功能从输入数据中提取特征如边缘角点纹理乃至更复杂的物体部件关键组件卷积核过滤器一个小型矩阵如其内部的数值是模型需要学习的参数不同的卷积核用于检测不同的特征滑动窗口卷积核在输入图像或上一层的特征图上从左到右从上到下依次滑动计算过程在每个位置卷积核与输入图像的对应区域进行点乘并求和得到一个单一的数值输出重要概念深度一层中使用的卷积核数量决定了输出特征图的深度每个卷积核都会生成一个独立的二维特征图步长卷积核每次滑动的像素数步长为则每次移动像素步长为则每次移动像素填充在输入数据的边缘填充一圈或其他值以控制输出特征图的空间尺寸输出卷积层输出的是一系列特征图每个图代表了某个特定特征在输入图像中不同位置的响应强度池化层降维与保持平移不变性功能对特征图进行下采样主要目的是减小数据尺寸降低计算量和内存消耗减少参数数量防止过拟合保持特征的尺度不变性和平移不变性使模型对输入图像的微小位移和形变不敏感常见类型最大池化在滑动窗口内取最大值这是最常用的方法能更好地保留纹理特征平均池化在滑动窗口内取平均值操作与卷积类似也需要指定窗口大小如和步长如一个步长为的池化层会将特征图的尺寸减小一半全连接层最终分类回归功能在经过多次卷积和池化后得到的高层特征图将被展平并输入到一个或多个全连接层中全连接层的作用是整合前面提取到的局部分布式特征并进行最终的分类或回归输出位置通常位于的末端输出对于分类任务最后一个全连接层通常使用激活函数输出每个类别的概率三的经典工作流程一个简化的流程可以概括为输入图像卷积层激活函数池化层展平全连接层输出结果激活函数在每个卷积层或全连接层之后通常会引入一个非线性激活函数如这是为了让网络能够学习和表示复杂的非线性关系模块重复卷积激活池化这个组合可以重复多次形成深度网络浅层网络学习低级特征边缘颜色深层网络则逐步组合这些低级特征形成更高级更抽象的特征眼睛鼻子整个物体为了直观理解这一流程下图展示了一个用于手写数字识别的经典例如的结构输入图像卷积层卷积核激活最大池化层步长卷积层卷积核激活最大池化层步长展平全连接层个神经元激活全连接层个神经元激活输出层个神经元输出数字的概率四经典架构由提出用于手写数字识别是的开山鼻祖在大赛上以巨大优势夺冠引发了深度学习革命它更深使用了和等技术通过反复堆叠的小卷积核构建了更深的网络证明了网络深度的重要性引入了模块在增加深度的同时控制了计算量提出了残差连接解决了极深网络的梯度消失和退化问题使得构建上百甚至上千层的网络成为可能五主要应用领域图像分类识别图像中的主要物体是什么如猫狗目标检测不仅识别物体还要定位出物体的位置用边界框标出图像分割对图像中的每个像素进行分类确定每个像素属于哪个物体或背景人脸识别识别或验证图像中人物的身份风格迁移将一幅图像的艺术风格应用到另一幅图像上图像生成通过生成对抗网络等生成新的图像总结卷积神经网络通过其独特的局部连接和权值共享机制高效地处理图像等网格数据它通过卷积层自动提取从低级到高级的特征利用池化层进行降维和增强鲁棒性最后通过全连接层完成最终任务的设计理念使其成为处理空间信息的最强有力的工具之一并持续推动着人工智能领域的发展循环神经网络进行一次全面而详细的介绍是一类专门设计用于处理序列数据的神经网络与传统的前馈神经网络如不同引入了记忆的概念使其能够捕捉数据中的时间动态信息一核心思想处理序列与引入记忆为什么需要对于许多任务数据的上下文和顺序至关重要例如理解一个句子苹果很好吃和苹果发布了新手机中的苹果含义完全不同需要根据前面的词来推断预测视频的下一帧需要知道之前的帧发生了什么股票价格的预测依赖于历史走势传统神经网络如全连接网络的局限它们假设所有的输入是独立的输入一个样本得到输出然后这个样本的信息就被忘记了处理下一个样本时网络不会记住之前的内容它们无法处理可变长度的序列输入的解决方案通过在网络内部引入循环连接来解决这个问题这使得网络能够将之前步骤的信息传递到当前步骤的处理中简单来说拥有一个内部状态或称隐藏状态这个状态就像是网络的记忆它包含了关于之前所有输入元素的摘要信息二的核心工作原理与结构循环单元展开视角理解最直观的方式是将其按时间序列展开我们以一个处理句子单词序列的为例关键组件在时间步的输入例如句子中的第个单词在时间步的隐藏状态这是网络的记忆它封装了到时间步为止的所有之前输入的信息在时间步的输出网络的权重参数请注意这些参数在所有时间步之间是共享的这是的一个关键特性极大地减少了需要学习的参数量并且允许处理不同长度的序列计算过程在每个时间步将当前输入和上一个隐藏状态结合起来通过一个激活函数通常是生成新的隐藏状态根据新的隐藏状态计算当前输出例如使用来预测下一个单词的概率信息流动隐藏状态作为信息传递的纽带通常初始化为零向量在处理发布了这个词时隐藏状态不仅依赖于还依赖于包含了苹果信息的因此能够捕捉序列中的上下文依赖关系不同的输入输出模式非常灵活可以配置成多种模式以适应不同任务一对一单一输入单一输出标准神经网络模式一对多单一输入序列输出如图像字幕输入图像输出描述性句子多对一序列输入单一输出如情感分析输入一个评论输出正面负面情感多对多同步序列输入序列输出每个时间步都有对应输出如视频帧级分类多对多异步序列输入序列输出如机器翻译编码器将整个输入序列编码为一个上下文向量解码器再将其解码为另一种语言的序列三的挑战与进阶变体基本的常被称为在实践中存在一个严重的问题难以学习长期依赖关系梯度消失与梯度爆炸问题原因在训练时我们使用随时间反向传播梯度需要从最后的输出层一路反向传播到早期的时间步这个过程涉及对相同权重矩阵的重复连乘梯度消失如果权重矩阵的特征值小于连乘会导致梯度指数级缩小到接近零这使得网络无法更新早期层的参数从而忘记了远距离的信息梯度爆炸如果权重矩阵的特征值大于连乘会导致梯度指数级增长造成训练不稳定长短期记忆网络是一种特殊的被明确设计来解决梯度消失问题能够有效地学习长期依赖关系的核心创新门控机制引入了三个门来精细地控制信息的流动遗忘门决定要从细胞状态长期记忆中丢弃哪些信息看看之前的隐藏状态和当前输入然后为细胞状态中的每个元素输出一个到之间的数表示完全保留表示完全遗忘输入门决定要将哪些新信息存入细胞状态首先一个候选值层创建一个新的候选值向量可能会被加入到状态中然后输入门决定我们要更新状态值的程度输出门决定基于当前的细胞状态输出什么到隐藏状态隐藏状态是细胞状态的过滤版本用于当前时间步的输出和传递给下一步细胞状态的关键是它的细胞状态它像一个传送带在整个链上运行只有一些线性的相互作用信息可以很容易地在其上保持不变地流动门控循环单元是的一个流行变体它将中的遗忘门和输入门合并为一个单一的更新门并混合了细胞状态和隐藏状态这使得的结构比更简单参数更少训练速度更快同时在许多任务上表现出与相当甚至更好的性能四的典型应用领域自然语言处理机器翻译早期文本生成情感分析语音识别时间序列预测股票价格预测天气预测音乐生成生成连贯的旋律视频分析对视频中的行为进行识别五的现状与未来虽然特别是和在序列建模上取得了巨大成功但近年来一种新的架构及其核心的自注意力机制在许多领域尤其是已经逐渐成为主流例如等模型优势完全摒弃了循环结构允许并行计算所有时间步的数据训练速度极快其自注意力机制能直接捕捉序列中任意两个位置之间的依赖关系无论距离多远从而更好地处理超长距离依赖的定位及其变体仍然是处理序列数据的重要工具尤其是在计算资源受限需要在线流式处理输入是连续的流或序列非常长以至于也难以处理的场景中它们为理解序列模型提供了根本性的思想总结循环神经网络通过其内部循环连接和隐藏状态赋予了网络记忆的能力使其成为处理序列数据的天然模型尽管基本的受困于梯度消失问题但其进阶变体和通过精巧的门控机制有效地解决了这一问题并在过去十年中推动了等领域的巨大发展理解是理解现代序列模型包括的重要基石',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-10-21 15:49:59',
  postMainColor: '',
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#18171d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#f7f9fe')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 6 || hour >= 18
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(e => {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 8.0.0"></head><body data-type="anzhiyu"><div id="web_bg"></div><div id="an_music_bg"></div><div id="loading-box" onclick="document.getElementById(&quot;loading-box&quot;).classList.add(&quot;loaded&quot;)"><div class="loading-bg"><img class="loading-img nolazyload" alt="加载头像" src="https://npm.elemecdn.com/anzhiyu-blog-static@1.0.4/img/avatar.jpg"/><div class="loading-image-dot"></div></div></div><script>const preloader = {
  endLoading: () => {
    document.getElementById('loading-box').classList.add("loaded");
  },
  initLoading: () => {
    document.getElementById('loading-box').classList.remove("loaded")
  }
}
window.addEventListener('load',()=> { preloader.endLoading() })
setTimeout(function(){preloader.endLoading();},10000)

if (true) {
  document.addEventListener('pjax:send', () => { preloader.initLoading() })
  document.addEventListener('pjax:complete', () => { preloader.endLoading() })
}</script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.10/progress_bar/progress_bar.css"/><script async="async" src="https://cdn.cbd.int/pace-js@1.2.4/pace.min.js" data-pace-options="{ &quot;restartOnRequestAfter&quot;:false,&quot;eventLag&quot;:false}"></script><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><div id="nav-group"><span id="blog_name"><div class="back-home-button"><i class="anzhiyufont anzhiyu-icon-grip-vertical"></i><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://hexo.anheyu.com/" title="博客"><img class="back-menu-item-icon" src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div></div><a id="site-name" href="/" accesskey="h"><div class="title">黎思敏的个人学习成长纪录</div><i class="anzhiyufont anzhiyu-icon-house-chimney"></i></a></span><div class="mask-name-container"><div id="name-container"><a id="page-name" href="javascript:anzhiyu.scrollToDest(0, 500)">PAGE_NAME</a></div></div><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div></div><div id="nav-right"><div class="nav-button" id="randomPost_button"><a class="site-page" onclick="toRandomPost()" title="随机前往一个文章" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-dice"></i></a></div><input id="center-console" type="checkbox"/><label class="widget" for="center-console" title="中控台" onclick="anzhiyu.switchConsole();"><i class="left"></i><i class="widget center"></i><i class="widget right"></i></label><div id="console"><div class="console-card-group-reward"><ul class="reward-all console-card"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" alt="微信" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" alt="支付宝" src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div><div class="console-card-group"><div class="console-card-group-left"><div class="console-card" id="card-newest-comments"><div class="card-content"><div class="author-content-item-tips">互动</div><span class="author-content-item-title"> 最新评论</span></div><div class="aside-list"><span>正在加载中...</span></div></div></div><div class="console-card-group-right"><div class="console-card tags"><div class="card-content"><div class="author-content-item-tips">兴趣点</div><span class="author-content-item-title">寻找你感兴趣的领域</span></div></div><div class="console-card history"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-box-archiv"></i><span>文章</span></div><div class="card-archives"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-archive"></i><span>归档</span></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/10/"><span class="card-archive-list-date">十月 2025</span><div class="card-archive-list-count-group"><span class="card-archive-list-count">21</span><span>篇</span></div></a></li></ul></div><hr/></div></div></div><div class="button-group"><div class="console-btn-item"><a class="darkmode_switchbutton" title="显示模式切换" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-moon"></i></a></div><div class="console-btn-item" id="consoleHideAside" onclick="anzhiyu.hideAsideBtn()" title="边栏显示控制"><a class="asideSwitch"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></a></div><div class="console-btn-item" id="consoleMusic" onclick="anzhiyu.musicToggle()" title="音乐开关"><a class="music-switch"><i class="anzhiyufont anzhiyu-icon-music"></i></a></div></div><div class="console-mask" onclick="anzhiyu.hideConsole()" href="javascript:void(0);"></div></div><div class="nav-button" id="nav-totop"><a class="totopbtn" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i><span id="percent" onclick="anzhiyu.scrollToDest(0,500)">0</span></a></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);" title="切换"><i class="anzhiyufont anzhiyu-icon-bars"></i></a></div></div></div></nav><div id="post-info"><div id="post-firstinfo"><div class="meta-firstline"><a class="post-meta-original">原创</a><span class="article-meta tags"></span></div></div><h1 class="post-title" itemprop="name headline">letnet</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="anzhiyufont anzhiyu-icon-calendar-days post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" itemprop="dateCreated datePublished" datetime="2025-10-07T05:21:33.000Z" title="发表于 2025-10-07 13:21:33">2025-10-07</time><span class="post-meta-separator"></span><i class="anzhiyufont anzhiyu-icon-history post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" itemprop="dateCreated datePublished" datetime="2025-10-21T07:49:59.337Z" title="更新于 2025-10-21 15:49:59">2025-10-21</time></span></div><div class="meta-secondline"><span class="post-meta-separator">       </span><span class="post-meta-position" title="作者IP属地为长沙"><i class="anzhiyufont anzhiyu-icon-location-dot"></i>长沙</span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section><div id="post-top-cover"><img class="nolazyload" id="post-top-bg" src=""></div></header><main id="blog-container"><div class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container" itemscope itemtype="http://example.com/2025/10/07/7.ViT/"><header><h1 id="CrawlerTitle" itemprop="name headline">letnet</h1><span itemprop="author" itemscope itemtype="http://schema.org/Person">黎思敏</span><time itemprop="dateCreated datePublished" datetime="2025-10-07T05:21:33.000Z" title="发表于 2025-10-07 13:21:33">2025-10-07</time><time itemprop="dateCreated datePublished" datetime="2025-10-21T07:49:59.337Z" title="更新于 2025-10-21 15:49:59">2025-10-21</time></header><h1 id="7-VIT"><a href="#7-VIT" class="headerlink" title="7.VIT"></a>7.VIT</h1><p><strong>Vision Transformer</strong>。这篇论文首次证明，<strong>纯Transformer架构无需卷积网络的辅助，直接在图像分类任务上就能达到顶尖水平</strong>，打破了计算机视觉长期被卷积神经网络统治的局面。</p>
<h3 id="论文核心信息"><a href="#论文核心信息" class="headerlink" title="论文核心信息"></a>论文核心信息</h3><ul>
<li><strong>标题</strong>: <strong>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</strong></li>
<li><strong>作者</strong>: Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, Neil Houlsby (Google Research)</li>
<li><strong>会议</strong>: ICLR 2021</li>
<li><strong>核心思想</strong>: 将图像分割成固定大小的图像块，将其线性嵌入后作为序列输入到标准的Transformer编码器中，进行图像分类。</li>
</ul>
<hr>
<h3 id="一、背景与动机：为什么要在视觉中用Transformer？"><a href="#一、背景与动机：为什么要在视觉中用Transformer？" class="headerlink" title="一、背景与动机：为什么要在视觉中用Transformer？"></a>一、背景与动机：为什么要在视觉中用Transformer？</h3><p>在ViT之前，计算机视觉几乎是<strong>卷积神经网络</strong> 的天下。从AlexNet到ResNet，CNN通过其<strong>局部连接</strong>和<strong>平移不变性</strong>等归纳偏置，在图像任务上取得了巨大成功。</p>
<p>与此同时，在自然语言处理领域，<strong>Transformer</strong> 凭借其<strong>自注意力机制</strong> 和<strong>全局建模能力</strong>，完全取代了RNN，成为绝对的主流。</p>
<p>一个很自然的问题是：<strong>Transformer这种强大的架构，能否在计算机视觉领域同样取得成功？</strong></p>
<p><strong>ViT的雄心</strong>：探索完全摒弃卷积操作，仅使用标准的Transformer架构来处理图像，并测试其性能极限。</p>
<hr>
<h3 id="二、模型架构：如何将图像转化为序列？"><a href="#二、模型架构：如何将图像转化为序列？" class="headerlink" title="二、模型架构：如何将图像转化为序列？"></a>二、模型架构：如何将图像转化为序列？</h3><p>Transformer是为序列数据（如单词序列）设计的，而图像是二维结构。ViT最核心、最创新的部分就在于它如何将图像转换为Transformer可以处理的序列。</p>
<h4 id="1-图像分块嵌入"><a href="#1-图像分块嵌入" class="headerlink" title="1. 图像分块嵌入"></a>1. 图像分块嵌入</h4><p>这是ViT模型的起点，其处理流程如下图所示：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/attachments/image-20251013125520538.png" alt="image-20251013125520538"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">flowchart TD</span><br><span class="line">    A[“输入图像&lt;br&gt;224x224x3”] --&gt; B[“分割为16x16图像块&lt;br&gt;得到14x14=196个块”]</span><br><span class="line">    B --&gt; C[“展平与线性投影&lt;br&gt;每个块变为D维向量”]</span><br><span class="line">    C --&gt; D[“添加可学习的&lt;br&gt;[class]令牌”]</span><br><span class="line">    D --&gt; E[“添加可学习的&lt;br&gt;1D位置编码”]</span><br><span class="line">    E --&gt; F[“Transformer编码器&lt;br&gt;的输入序列”]</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>图像分块</strong>：将输入图像（例如 224×224×3）分割成多个固定大小的图像块（例如 16×16）。这样，图像就被划分为 (224&#x2F;16)×(224&#x2F;16)&#x3D;14×14&#x3D;196个图像块。</li>
<li><strong>图像块展平与线性投影</strong>：将每个图像块（16×16×3&#x3D;768 维）展平为一个向量，然后通过一个可训练的<strong>线性投影层</strong>（全连接层）映射到模型指定的维度 D（例如 768768）。这个投影的输出被称为 <strong>“图像块嵌入”</strong>，其作用类似于NLP中的<strong>词嵌入</strong>。</li>
</ul>
<p><strong>这正是论文标题《一张图像等价于16x16个单词》的由来</strong>。模型将每个 16×16 的图像块视为一个”单词”。</p>
<h4 id="2-class-令牌与位置编码"><a href="#2-class-令牌与位置编码" class="headerlink" title="2. [class]令牌与位置编码"></a>2. [class]令牌与位置编码</h4><ul>
<li><strong>[class]令牌</strong>：在图像块嵌入序列的前面，预先添加一个可学习的嵌入向量。这个令牌的作用类似于BERT中的[CLS]令牌。在经过Transformer编码器处理后，其对应的输出状态将作为整个图像的<strong>全局表示</strong>，用于最终的分类。</li>
<li><strong>位置编码</strong>：由于Transformer本身是<strong>置换不变</strong>的，它无法感知序列中元素的顺序。但图像块的空间位置信息至关重要。因此，ViT为每个图像块嵌入添加一个<strong>可学习的一维位置编码</strong>，以保留图像块在原始图像中的位置信息。</li>
</ul>
<h4 id="3-Transformer编码器"><a href="#3-Transformer编码器" class="headerlink" title="3. Transformer编码器"></a>3. Transformer编码器</h4><p>将处理好的序列（[class]令牌 + 图像块嵌入 + 位置编码）输入到一个标准的<strong>Transformer编码器</strong>中。</p>
<p>Transformer编码器由 L个相同的层堆叠而成，每层包含：</p>
<ul>
<li><strong>多头自注意力机制</strong>：使每个图像块（包括[class]令牌）都能够与所有其他图像块进行交互，从而捕获全局的上下文信息。</li>
<li><strong>多层感知机</strong>：对每个位置的特征进行非线性变换。</li>
<li><strong>层归一化</strong>和<strong>残差连接</strong>：用于稳定和加速训练。</li>
</ul>
<p>最终，将[class]令牌对应的输出向量输入一个<strong>MLP分类头</strong>（通常是一个简单的线性层），得到最终的分类结果。</p>
<hr>
<h3 id="三、核心创新与关键发现"><a href="#三、核心创新与关键发现" class="headerlink" title="三、核心创新与关键发现"></a>三、核心创新与关键发现</h3><h4 id="1-摒弃卷积归纳偏置"><a href="#1-摒弃卷积归纳偏置" class="headerlink" title="1. 摒弃卷积归纳偏置"></a>1. 摒弃卷积归纳偏置</h4><p>CNN的成功很大程度上归功于其内置的<strong>归纳偏置</strong>，例如<strong>局部性</strong>（相邻像素关联性强）和<strong>平移不变性</strong>。ViT<strong>几乎没有任何针对图像的强归纳偏置</strong>。它只在最初的分块阶段隐含了局部性，在MLP层中是平移等变的，但自注意力层本身是全局的、置换敏感的。</p>
<p>这意味着，ViT必须<strong>完全从数据中学习</strong>所有关于图像空间结构的知识。</p>
<h4 id="2-尺度定律：数据量是关键"><a href="#2-尺度定律：数据量是关键" class="headerlink" title="2. 尺度定律：数据量是关键"></a>2. 尺度定律：数据量是关键</h4><p>论文最关键的发现是：<strong>ViT的性能强烈依赖于训练数据集的规模</strong>。</p>
<ul>
<li><strong>在中型数据集（如ImageNet）上训练</strong>：ViT的表现<strong>不如</strong>同等规模的CNN模型（如ResNet）。因为它缺乏CNN的归纳偏置，需要更多数据来学习这些视觉概念。</li>
<li><strong>在超大规模数据集（如JFT-300M，包含3亿张图像）上预训练</strong>：ViT的表现<strong>显著超越</strong>了最先进的CNN模型。这表明，当数据足够多时，Transformer强大的表示学习能力可以完全弥补其缺乏归纳偏置的弱点，甚至做得更好。</li>
</ul>
<h4 id="3-自注意力的全局视野"><a href="#3-自注意力的全局视野" class="headerlink" title="3. 自注意力的全局视野"></a>3. 自注意力的全局视野</h4><p>与CNN的局部感受野不同，ViT从第一层开始就拥有<strong>全局感受野</strong>。每个图像块都可以直接与任何其他图像块交互。这使得ViT能够更早、更有效地建模图像中长距离的依赖关系。</p>
<hr>
<h3 id="四、实验结果与影响"><a href="#四、实验结果与影响" class="headerlink" title="四、实验结果与影响"></a>四、实验结果与影响</h3><h4 id="1-主要结果"><a href="#1-主要结果" class="headerlink" title="1. 主要结果"></a>1. 主要结果</h4><ul>
<li><strong>ImageNet分类</strong>：在ImageNet上，最大的ViT模型（ViT-H&#x2F;14）达到了<strong>88.55%</strong> 的Top-1准确率，超越了当时最好的CNN模型。</li>
<li><strong>计算效率</strong>：在预训练时，ViT展现出优异的计算效率，在达到相似性能时，所需的预训练计算量远少于CNN。</li>
</ul>
<h4 id="2-历史意义"><a href="#2-历史意义" class="headerlink" title="2. 历史意义"></a>2. 历史意义</h4><ol>
<li><strong>打破了CNN的垄断</strong>：证明了纯Transformer架构在视觉任务上的巨大潜力，开辟了视觉研究的新方向。</li>
<li><strong>推动了视觉与语言的统一</strong>：由于NLP和CV可以使用相同的骨干网络，为后来的多模态模型（如CLIP、DALL·E）奠定了坚实的基础。</li>
<li><strong>催生了一系列ViT变体</strong>：如Swin Transformer（引入层次结构和局部性）、DeiT（无需大规模预训练）、MAE（掩码自编码器）等，形成了庞大的ViT家族。</li>
</ol>
<hr>
<h3 id="五、ViT的局限性"><a href="#五、ViT的局限性" class="headerlink" title="五、ViT的局限性"></a>五、ViT的局限性</h3><ol>
<li><strong>数据饥渴</strong>：严重依赖大规模预训练数据，对于没有海量数据的研究者不友好。</li>
<li><strong>计算复杂度高</strong>：自注意力的计算复杂度与序列长度的平方成正比。处理高分辨率图像时，图像块数量剧增，导致计算开销巨大。</li>
<li><strong>缺乏空间层次结构</strong>：原始ViT始终保持单一尺度的特征图，对需要多尺度特征的密集预测任务（如目标检测、分割）不友好。</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>Vision Transformer论文的核心贡献在于，它进行了一次<strong>大胆而成功的探索</strong>，将NLP中极其成功的<strong>标准Transformer架构</strong>几乎不做修改地应用到了图像领域。</p>
<ul>
<li><strong>它回答了什么问题？</strong> Transformer能否成为通用的视觉骨干网络？答案是<strong>可以</strong>，但前提是<strong>需要足够规模的数据</strong>。</li>
<li><strong>它的解决方案是什么？</strong> 将图像<strong>拆分为块序列</strong>，通过<strong>线性嵌入</strong>和<strong>位置编码</strong>将其转化为Transformer的输入格式。</li>
<li><strong>它带来了什么影响？</strong> 它打破了”卷积是视觉必备”的思维定式，引领了视觉Transformer的研究浪潮，并为构建统一的视觉-语言模型架构铺平了道路，是计算机视觉领域一个真正的范式转变。</li>
</ul>
<h3 id="卷积神经网络（Convolutional-Neural-Network-CNN）-进行一次全面而详细的介绍。"><a href="#卷积神经网络（Convolutional-Neural-Network-CNN）-进行一次全面而详细的介绍。" class="headerlink" title="卷积神经网络（Convolutional Neural Network, CNN） 进行一次全面而详细的介绍。"></a><strong>卷积神经网络（Convolutional Neural Network, CNN）</strong> 进行一次全面而详细的介绍。</h3><p>CNN是一种专门设计用于处理<strong>具有网格状拓扑结构数据</strong>（如图像、视频、音频频谱图）的深度学习模型。它在计算机视觉领域取得了革命性的成功，是当今图像识别、目标检测、图像分割等技术的核心基础。</p>
<hr>
<h4 id="一、-核心思想与两大优势"><a href="#一、-核心思想与两大优势" class="headerlink" title="一、 核心思想与两大优势"></a><strong>一、 核心思想与两大优势</strong></h4><p>传统的全连接神经网络在处理图像时，会将三维（高度×宽度×通道）的图像展平为一维向量，这会导致两个严重问题：</p>
<ol>
<li><strong>参数爆炸：</strong> 一张1000x1000像素的图片，展平后输入层就有100万个节点，如果第一个隐藏层也有100万个节点，那么仅这一层的参数就高达1万亿个。这极大地增加了计算负担和过拟合风险。</li>
<li><strong>忽略空间结构：</strong> 展平操作破坏了像素在空间上的邻近关系，而图像中的物体正是由这些在空间上相邻的像素构成的。</li>
</ol>
<p>CNN通过两种核心思想解决了以上问题：</p>
<ul>
<li><strong>局部感受野：</strong> 每个神经元只与前一层的一个小区域（局部区域）相连，而不是与所有神经元全连接。这模拟了生物视觉系统中视网膜细胞只感受特定区域刺激的特性。</li>
<li><strong>参数共享：</strong> 同一个特征检测器（卷积核）在输入数据的整个空间范围内滑动，重复使用相同的参数。这极大地减少了模型参数。</li>
</ul>
<p>这两大思想具体通过 <strong>“卷积”</strong> 操作来实现。</p>
<hr>
<h4 id="二、-CNN的核心组成模块"><a href="#二、-CNN的核心组成模块" class="headerlink" title="二、 CNN的核心组成模块"></a><strong>二、 CNN的核心组成模块</strong></h4><p>一个典型的CNN由一系列层堆叠而成，主要包括以下三种核心层：</p>
<h4 id="1-卷积层-特征提取的核心"><a href="#1-卷积层-特征提取的核心" class="headerlink" title="1. 卷积层 - 特征提取的核心"></a><strong>1. 卷积层 - 特征提取的核心</strong></h4><ul>
<li><strong>功能：</strong> 从输入数据中提取特征，如边缘、角点、纹理、乃至更复杂的物体部件。</li>
<li><strong>关键组件：</strong><ul>
<li><strong>卷积核 &#x2F; 过滤器：</strong> 一个小型矩阵（如3x3, 5x5），其内部的数值是模型需要学习的参数。不同的卷积核用于检测不同的特征。</li>
<li><strong>滑动窗口：</strong> 卷积核在输入图像（或上一层的特征图）上从左到右、从上到下依次滑动。</li>
<li><strong>计算过程：</strong> 在每个位置，卷积核与输入图像的对应区域进行<strong>点乘</strong>并求和，得到一个单一的数值输出。</li>
</ul>
</li>
<li><strong>重要概念：</strong><ul>
<li><strong>深度：</strong> 一层中使用的卷积核数量决定了输出特征图的深度。每个卷积核都会生成一个独立的二维特征图。</li>
<li><strong>步长：</strong> 卷积核每次滑动的像素数。步长为1则每次移动1像素，步长为2则每次移动2像素。</li>
<li><strong>填充：</strong> 在输入数据的边缘填充一圈0（或其他值），以控制输出特征图的空间尺寸。</li>
</ul>
</li>
<li><strong>输出：</strong> 卷积层输出的是一系列<strong>特征图</strong>，每个图代表了某个特定特征在输入图像中不同位置的响应强度。</li>
</ul>
<h4 id="2-池化层-降维与保持平移不变性"><a href="#2-池化层-降维与保持平移不变性" class="headerlink" title="2. 池化层 - 降维与保持平移不变性"></a><strong>2. 池化层 - 降维与保持平移不变性</strong></h4><ul>
<li><strong>功能：</strong> 对特征图进行<strong>下采样</strong>，主要目的是：<ol>
<li><strong>减小数据尺寸</strong>，降低计算量和内存消耗。</li>
<li><strong>减少参数数量</strong>，防止过拟合。</li>
<li><strong>保持特征的尺度不变性和平移不变性</strong>，使模型对输入图像的微小位移和形变不敏感。</li>
</ol>
</li>
<li><strong>常见类型：</strong><ul>
<li><strong>最大池化：</strong> 在滑动窗口内取最大值。这是最常用的方法，能更好地保留纹理特征。</li>
<li><strong>平均池化：</strong> 在滑动窗口内取平均值。</li>
</ul>
</li>
<li><strong>操作：</strong> 与卷积类似，也需要指定窗口大小（如2x2）和步长（如2）。一个2x2，步长为2的池化层会将特征图的尺寸减小一半。</li>
</ul>
<h4 id="3-全连接层-最终分类-回归"><a href="#3-全连接层-最终分类-回归" class="headerlink" title="3. 全连接层 - 最终分类&#x2F;回归"></a><strong>3. 全连接层 - 最终分类&#x2F;回归</strong></h4><ul>
<li><strong>功能：</strong> 在经过多次卷积和池化后，得到的高层特征图将被展平，并输入到一个或多个全连接层中。全连接层的作用是<strong>整合前面提取到的局部、分布式特征</strong>，并进行最终的分类或回归输出。</li>
<li><strong>位置：</strong> 通常位于CNN的末端。</li>
<li><strong>输出：</strong> 对于分类任务，最后一个全连接层通常使用Softmax激活函数，输出每个类别的概率。</li>
</ul>
<hr>
<h4 id="三、-CNN的经典工作流程"><a href="#三、-CNN的经典工作流程" class="headerlink" title="三、 CNN的经典工作流程"></a><strong>三、 CNN的经典工作流程</strong></h4><p>一个简化的CNN流程可以概括为：<br><strong>输入图像 → [卷积层 → 激活函数 → 池化层] × N → 展平 → 全连接层 → 输出结果</strong></p>
<ul>
<li><strong>激活函数：</strong> 在每个卷积层或全连接层之后，通常会引入一个非线性激活函数，如 <strong>ReLU</strong>，这是为了让网络能够学习和表示复杂的非线性关系。</li>
<li><strong>模块重复：</strong> <code>[卷积 → 激活 → 池化]</code> 这个组合可以重复多次，形成深度网络。浅层网络学习低级特征（边缘、颜色），深层网络则逐步组合这些低级特征，形成更高级、更抽象的特征（眼睛、鼻子、整个物体）。</li>
</ul>
<p>为了直观理解这一流程，下图展示了一个用于手写数字识别的经典CNN（例如LeNet-5）的结构：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/attachments/image-20251014110241504.png" alt="image-20251014110241504"></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">flowchart TD</span><br><span class="line">    A[“输入图像&lt;br&gt;28x28x1”] --&gt; B[“卷积层&lt;br&gt;5x5x6卷积核”]</span><br><span class="line">    B --&gt; C[“ReLU激活”]</span><br><span class="line">    C --&gt; D[“最大池化层&lt;br&gt;2x2， 步长2”]</span><br><span class="line">    D --&gt; E[“卷积层&lt;br&gt;5x5x16卷积核”]</span><br><span class="line">    E --&gt; F[“ReLU激活”]</span><br><span class="line">    F --&gt; G[“最大池化层&lt;br&gt;2x2， 步长2”]</span><br><span class="line">    G --&gt; H[“展平”]</span><br><span class="line">    H --&gt; I[“全连接层&lt;br&gt;120个神经元”]</span><br><span class="line">    I --&gt; J[“ReLU激活”]</span><br><span class="line">    J --&gt; K[“全连接层&lt;br&gt;84个神经元”]</span><br><span class="line">    K --&gt; L[“ReLU激活”]</span><br><span class="line">    L --&gt; M[“输出层&lt;br&gt;10个神经元 + Softmax”]</span><br><span class="line">    M --&gt; N[“输出: 数字0-9的概率”]</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="四、-经典CNN架构"><a href="#四、-经典CNN架构" class="headerlink" title="四、 经典CNN架构"></a><strong>四、 经典CNN架构</strong></h4><ol>
<li><strong>LeNet-5 (1998):</strong> 由Yann LeCun提出，用于手写数字识别，是CNN的开山鼻祖。</li>
<li><strong>AlexNet (2012):</strong> 在ImageNet大赛上以巨大优势夺冠，引发了深度学习革命。它更深，使用了ReLU和Dropout等技术。</li>
<li><strong>VGGNet (2014):</strong> 通过反复堆叠3x3的小卷积核，构建了更深的网络，证明了网络深度的重要性。</li>
<li><strong>GoogLeNet (2014):</strong> 引入了<strong>Inception模块</strong>，在增加深度的同时控制了计算量。</li>
<li><strong>ResNet (2015):</strong> 提出了<strong>残差连接</strong>，解决了极深网络的梯度消失和退化问题，使得构建上百甚至上千层的网络成为可能。</li>
</ol>
<hr>
<h4 id="五、-主要应用领域"><a href="#五、-主要应用领域" class="headerlink" title="五、 主要应用领域"></a><strong>五、 主要应用领域</strong></h4><ul>
<li><strong>图像分类：</strong> 识别图像中的主要物体是什么（如“猫”、“狗”）。</li>
<li><strong>目标检测：</strong> 不仅识别物体，还要定位出物体的位置（用边界框标出）。</li>
<li><strong>图像分割：</strong> 对图像中的每个像素进行分类，确定每个像素属于哪个物体或背景。</li>
<li><strong>人脸识别：</strong> 识别或验证图像中人物的身份。</li>
<li><strong>风格迁移：</strong> 将一幅图像的艺术风格应用到另一幅图像上。</li>
<li><strong>图像生成：</strong> 通过生成对抗网络（GAN）等生成新的图像。</li>
</ul>
<h4 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a><strong>总结</strong></h4><p>卷积神经网络通过其独特的<strong>局部连接</strong>和<strong>权值共享</strong>机制，高效地处理图像等网格数据。它通过<strong>卷积层</strong>自动提取从低级到高级的特征，利用<strong>池化层</strong>进行降维和增强鲁棒性，最后通过<strong>全连接层</strong>完成最终任务。CNN的设计理念使其成为处理空间信息的<strong>最强有力的工具之一</strong>，并持续推动着人工智能领域的发展。</p>
<h3 id="循环神经网络（Recurrent-Neural-Network-RNN）-进行一次全面而详细的介绍。"><a href="#循环神经网络（Recurrent-Neural-Network-RNN）-进行一次全面而详细的介绍。" class="headerlink" title="循环神经网络（Recurrent Neural Network, RNN） 进行一次全面而详细的介绍。"></a><strong>循环神经网络（Recurrent Neural Network, RNN）</strong> 进行一次全面而详细的介绍。</h3><p>RNN是一类专门设计用于处理<strong>序列数据</strong>的神经网络。与传统的前馈神经网络（如CNN）不同，RNN引入了“记忆”的概念，使其能够捕捉数据中的时间动态信息。</p>
<hr>
<h4 id="一、-核心思想：处理序列与引入“记忆”"><a href="#一、-核心思想：处理序列与引入“记忆”" class="headerlink" title="一、 核心思想：处理序列与引入“记忆”"></a><strong>一、 核心思想：处理序列与引入“记忆”</strong></h4><p><strong>为什么需要RNN？</strong><br>对于许多任务，数据的上下文和顺序至关重要。例如：</p>
<ul>
<li>理解一个句子：“苹果很好吃。” 和 “苹果发布了新手机。” 中的“苹果”含义完全不同，需要根据前面的词来推断。</li>
<li>预测视频的下一帧，需要知道之前的帧发生了什么。</li>
<li>股票价格的预测依赖于历史走势。</li>
</ul>
<p><strong>传统神经网络（如CNN、全连接网络）的局限：</strong><br>它们假设所有的输入是<strong>独立</strong>的。输入一个样本，得到输出，然后这个样本的信息就被“忘记”了。处理下一个样本时，网络不会“记住”之前的内容。它们无法处理可变长度的序列输入。</p>
<p><strong>RNN的解决方案：</strong><br>RNN通过在网络内部引入<strong>循环连接</strong>来解决这个问题。这使得网络能够将<strong>之前步骤的信息</strong>传递到<strong>当前步骤</strong>的处理中。简单来说，RNN拥有一个“内部状态”（或称“隐藏状态”），这个状态就像是网络的<strong>记忆</strong>，它包含了关于之前所有输入元素的摘要信息。</p>
<hr>
<h4 id="二、-RNN的核心工作原理与结构"><a href="#二、-RNN的核心工作原理与结构" class="headerlink" title="二、 RNN的核心工作原理与结构"></a><strong>二、 RNN的核心工作原理与结构</strong></h4><h4 id="1-循环单元：展开视角"><a href="#1-循环单元：展开视角" class="headerlink" title="1. 循环单元：展开视角"></a><strong>1. 循环单元：展开视角</strong></h4><p>理解RNN最直观的方式是将其<strong>按时间序列展开</strong>。我们以一个处理句子（单词序列）的RNN为例：</p>
<ul>
<li><p><strong>关键组件：</strong></p>
<ul>
<li>xt： 在时间步 t 的<strong>输入</strong>。（例如，句子中的第t个单词）</li>
<li>ht： 在时间步 t 的<strong>隐藏状态</strong>。这是网络的“记忆”，它封装了到时间步 t<em>t</em> 为止的所有之前输入的信息。</li>
<li>yt： 在时间步 t 的<strong>输出</strong>。</li>
<li>W,U,V： 网络的<strong>权重参数</strong>。请注意，<strong>这些参数在所有时间步之间是共享的</strong>。这是RNN的一个关键特性，极大地减少了需要学习的参数量，并且允许处理不同长度的序列。</li>
</ul>
</li>
<li><p><strong>计算过程（在每个时间步 t）：</strong></p>
<ol>
<li><p>将当前输入 xt 和<strong>上一个隐藏状态</strong> ht−1结合起来。</p>
</li>
<li><p>通过一个激活函数（通常是tanh）生成新的隐藏状态 ht。<br>$$<br>h_t &#x3D; \tanh(W \cdot h_{t-1} + U \cdot x_t + b_h)<br>$$</p>
</li>
<li><p>根据新的隐藏状态 ht计算当前输出 yt（例如，使用Softmax来预测下一个单词的概率）。<br>$$<br>y_t &#x3D; Softmax(V \cdot h_t + b_y)<br>$$</p>
</li>
</ol>
</li>
<li><p><strong>信息流动：</strong></p>
<ul>
<li>隐藏状态 ht 作为信息传递的“纽带”。h0 通常初始化为零向量。</li>
<li>在处理“发布了”这个词（x2）时，隐藏状态 h2 不仅依赖于 x2，还依赖于包含了“苹果”信息的 h1。</li>
<li>因此，RNN能够<strong>捕捉序列中的上下文依赖关系</strong>。</li>
</ul>
</li>
</ul>
<h4 id="2-不同的输入输出模式"><a href="#2-不同的输入输出模式" class="headerlink" title="2. 不同的输入输出模式"></a><strong>2. 不同的输入输出模式</strong></h4><p>RNN非常灵活，可以配置成多种模式以适应不同任务：</p>
<ol>
<li><strong>一对一：</strong> 单一输入，单一输出。（标准神经网络模式）</li>
<li><strong>一对多：</strong> 单一输入，序列输出。（如图像字幕：输入图像，输出描述性句子）</li>
<li><strong>多对一：</strong> 序列输入，单一输出。（如情感分析：输入一个评论，输出正面&#x2F;负面情感）</li>
<li><strong>多对多（同步）：</strong> 序列输入，序列输出，每个时间步都有对应输出。（如视频帧级分类）</li>
<li><strong>多对多（异步）：</strong> 序列输入，序列输出。（如机器翻译：编码器将整个输入序列编码为一个上下文向量，解码器再将其解码为另一种语言的序列）</li>
</ol>
<hr>
<h4 id="三、-RNN的挑战与进阶变体"><a href="#三、-RNN的挑战与进阶变体" class="headerlink" title="三、 RNN的挑战与进阶变体"></a><strong>三、 RNN的挑战与进阶变体</strong></h4><p>基本的RNN（常被称为“Vanilla RNN”）在实践中存在一个严重的问题：<strong>难以学习长期依赖关系</strong>。</p>
<h5 id="1-梯度消失与梯度爆炸问题"><a href="#1-梯度消失与梯度爆炸问题" class="headerlink" title="1. 梯度消失与梯度爆炸问题"></a><strong>1. 梯度消失与梯度爆炸问题</strong></h5><ul>
<li><strong>原因：</strong> 在训练RNN时，我们使用<strong>随时间反向传播</strong>。梯度需要从最后的输出层一路反向传播到早期的时间步。这个过程涉及对相同权重矩阵的<strong>重复连乘</strong>。</li>
<li><strong>梯度消失：</strong> 如果权重矩阵的特征值小于1，连乘会导致梯度指数级缩小到接近零。这使得网络无法更新早期层的参数，从而“忘记”了远距离的信息。</li>
<li><strong>梯度爆炸：</strong> 如果权重矩阵的特征值大于1，连乘会导致梯度指数级增长，造成训练不稳定。</li>
</ul>
<h5 id="2-长短期记忆网络（LSTM）"><a href="#2-长短期记忆网络（LSTM）" class="headerlink" title="2. 长短期记忆网络（LSTM）"></a><strong>2. 长短期记忆网络（LSTM）</strong></h5><p>LSTM是一种特殊的RNN，被明确设计来解决梯度消失问题，能够有效地学习长期依赖关系。</p>
<p><strong>LSTM的核心创新：门控机制</strong><br>LSTM引入了三个“门”来精细地控制信息的流动：</p>
<ul>
<li><strong>遗忘门：</strong> 决定要从细胞状态（长期记忆）中<strong>丢弃</strong>哪些信息。<ul>
<li>“看看之前的隐藏状态和当前输入，然后为细胞状态 Ct−1 中的每个元素输出一个0到1之间的数。1表示‘完全保留’，0表示‘完全遗忘’。”</li>
</ul>
</li>
<li><strong>输入门：</strong> 决定要将<strong>哪些新信息</strong>存入细胞状态。<ul>
<li>首先，一个“候选值”层创建一个新的候选值向量 C~t，可能会被加入到状态中。</li>
<li>然后，输入门决定我们要更新状态值的程度。</li>
</ul>
</li>
<li><strong>输出门：</strong> 决定基于当前的细胞状态<strong>输出</strong>什么到隐藏状态 ht。<ul>
<li>隐藏状态 ht 是细胞状态 Ct 的过滤版本，用于当前时间步的输出和传递给下一步。</li>
</ul>
</li>
</ul>
<p><strong>细胞状态：</strong> LSTM的关键是它的<strong>细胞状态</strong>，它像一个“传送带”，在整个链上运行，只有一些线性的相互作用，信息可以很容易地在其上保持不变地流动。</p>
<h5 id="3-门控循环单元（GRU）"><a href="#3-门控循环单元（GRU）" class="headerlink" title="3. 门控循环单元（GRU）"></a><strong>3. 门控循环单元（GRU）</strong></h5><p>GRU是LSTM的一个流行变体，它将LSTM中的遗忘门和输入门合并为一个单一的“更新门”，并混合了细胞状态和隐藏状态。这使得GRU的结构比LSTM更简单，参数更少，训练速度更快，同时在许多任务上表现出与LSTM相当甚至更好的性能。</p>
<hr>
<h4 id="四、-RNN的典型应用领域"><a href="#四、-RNN的典型应用领域" class="headerlink" title="四、 RNN的典型应用领域"></a><strong>四、 RNN的典型应用领域</strong></h4><ul>
<li><strong>自然语言处理：</strong><ul>
<li>机器翻译（早期）</li>
<li>文本生成</li>
<li>情感分析</li>
<li>语音识别</li>
</ul>
</li>
<li><strong>时间序列预测：</strong><ul>
<li>股票价格预测</li>
<li>天气预测</li>
</ul>
</li>
<li><strong>音乐生成：</strong> 生成连贯的旋律。</li>
<li><strong>视频分析：</strong> 对视频中的行为进行识别。</li>
</ul>
<hr>
<h4 id="五、-RNN的现状与未来"><a href="#五、-RNN的现状与未来" class="headerlink" title="五、 RNN的现状与未来"></a><strong>五、 RNN的现状与未来</strong></h4><p>虽然RNN（特别是LSTM和GRU）在序列建模上取得了巨大成功，但近年来，一种新的架构——<strong>Transformer</strong> 及其核心的<strong>自注意力机制</strong>——在许多领域（尤其是NLP）已经逐渐成为主流（例如BERT、GPT等模型）。</p>
<p><strong>Transformer vs. RNN:</strong></p>
<ul>
<li><strong>优势：</strong> Transformer完全摒弃了循环结构，允许<strong>并行计算</strong>所有时间步的数据，训练速度极快。其自注意力机制能直接捕捉序列中任意两个位置之间的依赖关系，无论距离多远，从而更好地处理<strong>超长距离依赖</strong>。</li>
<li><strong>RNN的定位：</strong> RNN及其变体（LSTM&#x2F;GRU）仍然是处理序列数据的重要工具，尤其是在<strong>计算资源受限</strong>、需要<strong>在线流式处理</strong>（输入是连续的流）或<strong>序列非常长</strong>以至于Transformer也难以处理的场景中。它们为理解序列模型提供了根本性的思想。</li>
</ul>
<h4 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a><strong>总结</strong></h4><p>循环神经网络通过其<strong>内部循环连接</strong>和<strong>隐藏状态</strong>，赋予了网络“记忆”的能力，使其成为处理序列数据的天然模型。尽管基本的RNN受困于<strong>梯度消失问题</strong>，但其进阶变体<strong>LSTM</strong>和<strong>GRU</strong>通过精巧的<strong>门控机制</strong>有效地解决了这一问题，并在过去十年中推动了NLP等领域的巨大发展。理解RNN是理解现代序列模型（包括Transformer）的重要基石。</p>
</article><div class="post-copyright"><div class="copyright-cc-box"><i class="anzhiyufont anzhiyu-icon-copyright"></i></div><div class="post-copyright__author_box"><a class="post-copyright__author_img" href="/" title="头像"><img class="post-copyright__author_img_back" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"><img class="post-copyright__author_img_front" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" title="头像" alt="头像"></a><div class="post-copyright__author_name">黎思敏</div><div class="post-copyright__author_desc">Li Simin’s Personal Blog</div></div><div class="post-copyright__post__info"><a class="post-copyright__original" title="该文章为原创文章，注意版权协议" href="http://example.com/2025/10/07/7.ViT/">原创</a><a class="post-copyright-title"><span onclick="rm.copyPageUrl('http://example.com/2025/10/07/7.ViT/')">letnet</span></a></div><div class="post-tools" id="post-tools"><div class="post-tools-left"><div class="rewardLeftButton"><div class="post-reward" onclick="anzhiyu.addRewardMask()"><div class="reward-button button--animated" title="赞赏作者"><i class="anzhiyufont anzhiyu-icon-hand-heart-fill"></i>打赏作者</div><div class="reward-main"><div class="reward-all"><span class="reward-title">感谢你赐予我前进的力量</span><ul class="reward-group"><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-weichat.png" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/qrcode-alipay.png" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul><a class="reward-main-btn" href="/about/#about-reward" target="_blank"><div class="reward-text">赞赏者名单</div><div class="reward-dec">因为你们的支持让我意识到写文章的价值🙏</div></a></div></div></div><div id="quit-box" onclick="anzhiyu.removeRewardMask()" style="display: none"></div></div><div class="shareRight"><div class="share-link mobile"><div class="share-qrcode"><div class="share-button" title="使用手机访问这篇文章"><i class="anzhiyufont anzhiyu-icon-qrcode"></i></div><div class="share-main"><div class="share-main-all"><div id="qrcode" title="http://example.com/2025/10/07/7.ViT/"></div><div class="reward-dec">使用手机访问这篇文章</div></div></div></div></div><div class="share-link weibo"><a class="share-button" target="_blank" href="https://service.weibo.com/share/share.php?title=letnet&amp;url=http://example.com/2025/10/07/7.ViT/&amp;pic=" rel="external nofollow noreferrer noopener"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a></div><script>function copyCurrentPageUrl() {
  var currentPageUrl = window.location.href;
  var input = document.createElement("input");
  input.setAttribute("value", currentPageUrl);
  document.body.appendChild(input);
  input.select();
  input.setSelectionRange(0, 99999);
  document.execCommand("copy");
  document.body.removeChild(input);
}</script><div class="share-link copyurl"><div class="share-button" id="post-share-url" title="复制链接" onclick="copyCurrentPageUrl()"><i class="anzhiyufont anzhiyu-icon-link"></i></div></div></div></div></div><div class="post-copyright__notice"><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://example.com" target="_blank">黎思敏的个人学习成长纪录</a>！</span></div></div><div class="post-tools-right"><div class="tag_share"><div class="post-meta__box"><div class="post-meta__box__tag-list"></div></div></div><div class="post_share"><div class="social-share" data-image="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"/><script src="https://cdn.cbd.int/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer="defer"></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/10/06/6.BERT/"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">letnet</div></div></a></div><div class="next-post pull-right"><a href="/2025/10/08/8.MAE/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">letnet</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="card-content"><div class="author-info__sayhi" id="author-info__sayhi" onclick="anzhiyu.changeSayHelloText()"></div><div class="author-info-avatar"><img class="avatar-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/><div class="author-status"><img class="g-status" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/08/24/64e6ce9c507bb.png" alt="status"/></div></div><div class="author-info__description">黎思敏的个人学习成长纪录博客</div></div></div><div class="card-widget anzhiyu-right-widget" id="card-wechat" onclick="null"><div id="flip-wrapper"><div id="flip-content"><div class="face" style="background: url(https://bu.dusays.com/2023/01/13/63c02edf44033.png) center center / 100% no-repeat"></div><div class="back face" style="background: url(https://bu.dusays.com/2023/05/13/645fa415e8694.png) center center / 100% no-repeat"></div></div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-bars"></i><span>文章目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#7-VIT"><span class="toc-number">1.</span> <span class="toc-text">7.VIT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%A0%B8%E5%BF%83%E4%BF%A1%E6%81%AF"><span class="toc-number">1.0.1.</span> <span class="toc-text">论文核心信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E3%80%81%E8%83%8C%E6%99%AF%E4%B8%8E%E5%8A%A8%E6%9C%BA%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%9C%A8%E8%A7%86%E8%A7%89%E4%B8%AD%E7%94%A8Transformer%EF%BC%9F"><span class="toc-number">1.0.2.</span> <span class="toc-text">一、背景与动机：为什么要在视觉中用Transformer？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%8C%E3%80%81%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%EF%BC%9A%E5%A6%82%E4%BD%95%E5%B0%86%E5%9B%BE%E5%83%8F%E8%BD%AC%E5%8C%96%E4%B8%BA%E5%BA%8F%E5%88%97%EF%BC%9F"><span class="toc-number">1.0.3.</span> <span class="toc-text">二、模型架构：如何将图像转化为序列？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%9B%BE%E5%83%8F%E5%88%86%E5%9D%97%E5%B5%8C%E5%85%A5"><span class="toc-number">1.0.3.1.</span> <span class="toc-text">1. 图像分块嵌入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-class-%E4%BB%A4%E7%89%8C%E4%B8%8E%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81"><span class="toc-number">1.0.3.2.</span> <span class="toc-text">2. [class]令牌与位置编码</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-Transformer%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-number">1.0.3.3.</span> <span class="toc-text">3. Transformer编码器</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E3%80%81%E6%A0%B8%E5%BF%83%E5%88%9B%E6%96%B0%E4%B8%8E%E5%85%B3%E9%94%AE%E5%8F%91%E7%8E%B0"><span class="toc-number">1.0.4.</span> <span class="toc-text">三、核心创新与关键发现</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E6%91%92%E5%BC%83%E5%8D%B7%E7%A7%AF%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE"><span class="toc-number">1.0.4.1.</span> <span class="toc-text">1. 摒弃卷积归纳偏置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%B0%BA%E5%BA%A6%E5%AE%9A%E5%BE%8B%EF%BC%9A%E6%95%B0%E6%8D%AE%E9%87%8F%E6%98%AF%E5%85%B3%E9%94%AE"><span class="toc-number">1.0.4.2.</span> <span class="toc-text">2. 尺度定律：数据量是关键</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E5%85%A8%E5%B1%80%E8%A7%86%E9%87%8E"><span class="toc-number">1.0.4.3.</span> <span class="toc-text">3. 自注意力的全局视野</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9B%E3%80%81%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E4%B8%8E%E5%BD%B1%E5%93%8D"><span class="toc-number">1.0.5.</span> <span class="toc-text">四、实验结果与影响</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C"><span class="toc-number">1.0.5.1.</span> <span class="toc-text">1. 主要结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E5%8E%86%E5%8F%B2%E6%84%8F%E4%B9%89"><span class="toc-number">1.0.5.2.</span> <span class="toc-text">2. 历史意义</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%94%E3%80%81ViT%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-number">1.0.6.</span> <span class="toc-text">五、ViT的局限性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">1.0.7.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Convolutional-Neural-Network-CNN%EF%BC%89-%E8%BF%9B%E8%A1%8C%E4%B8%80%E6%AC%A1%E5%85%A8%E9%9D%A2%E8%80%8C%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BB%8B%E7%BB%8D%E3%80%82"><span class="toc-number">1.0.8.</span> <span class="toc-text">卷积神经网络（Convolutional Neural Network, CNN） 进行一次全面而详细的介绍。</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%E4%B8%8E%E4%B8%A4%E5%A4%A7%E4%BC%98%E5%8A%BF"><span class="toc-number">1.0.8.1.</span> <span class="toc-text">一、 核心思想与两大优势</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81-CNN%E7%9A%84%E6%A0%B8%E5%BF%83%E7%BB%84%E6%88%90%E6%A8%A1%E5%9D%97"><span class="toc-number">1.0.8.2.</span> <span class="toc-text">二、 CNN的核心组成模块</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%8D%B7%E7%A7%AF%E5%B1%82-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E7%9A%84%E6%A0%B8%E5%BF%83"><span class="toc-number">1.0.8.3.</span> <span class="toc-text">1. 卷积层 - 特征提取的核心</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E6%B1%A0%E5%8C%96%E5%B1%82-%E9%99%8D%E7%BB%B4%E4%B8%8E%E4%BF%9D%E6%8C%81%E5%B9%B3%E7%A7%BB%E4%B8%8D%E5%8F%98%E6%80%A7"><span class="toc-number">1.0.8.4.</span> <span class="toc-text">2. 池化层 - 降维与保持平移不变性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82-%E6%9C%80%E7%BB%88%E5%88%86%E7%B1%BB-%E5%9B%9E%E5%BD%92"><span class="toc-number">1.0.8.5.</span> <span class="toc-text">3. 全连接层 - 最终分类&#x2F;回归</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E3%80%81-CNN%E7%9A%84%E7%BB%8F%E5%85%B8%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">1.0.8.6.</span> <span class="toc-text">三、 CNN的经典工作流程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B%E3%80%81-%E7%BB%8F%E5%85%B8CNN%E6%9E%B6%E6%9E%84"><span class="toc-number">1.0.8.7.</span> <span class="toc-text">四、 经典CNN架构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E3%80%81-%E4%B8%BB%E8%A6%81%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-number">1.0.8.8.</span> <span class="toc-text">五、 主要应用领域</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">1.0.8.9.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%EF%BC%88Recurrent-Neural-Network-RNN%EF%BC%89-%E8%BF%9B%E8%A1%8C%E4%B8%80%E6%AC%A1%E5%85%A8%E9%9D%A2%E8%80%8C%E8%AF%A6%E7%BB%86%E7%9A%84%E4%BB%8B%E7%BB%8D%E3%80%82"><span class="toc-number">1.0.9.</span> <span class="toc-text">循环神经网络（Recurrent Neural Network, RNN） 进行一次全面而详细的介绍。</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%80%E3%80%81-%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3%EF%BC%9A%E5%A4%84%E7%90%86%E5%BA%8F%E5%88%97%E4%B8%8E%E5%BC%95%E5%85%A5%E2%80%9C%E8%AE%B0%E5%BF%86%E2%80%9D"><span class="toc-number">1.0.9.1.</span> <span class="toc-text">一、 核心思想：处理序列与引入“记忆”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%8C%E3%80%81-RNN%E7%9A%84%E6%A0%B8%E5%BF%83%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%E4%B8%8E%E7%BB%93%E6%9E%84"><span class="toc-number">1.0.9.2.</span> <span class="toc-text">二、 RNN的核心工作原理与结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%EF%BC%9A%E5%B1%95%E5%BC%80%E8%A7%86%E8%A7%92"><span class="toc-number">1.0.9.3.</span> <span class="toc-text">1. 循环单元：展开视角</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-%E4%B8%8D%E5%90%8C%E7%9A%84%E8%BE%93%E5%85%A5%E8%BE%93%E5%87%BA%E6%A8%A1%E5%BC%8F"><span class="toc-number">1.0.9.4.</span> <span class="toc-text">2. 不同的输入输出模式</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E3%80%81-RNN%E7%9A%84%E6%8C%91%E6%88%98%E4%B8%8E%E8%BF%9B%E9%98%B6%E5%8F%98%E4%BD%93"><span class="toc-number">1.0.9.5.</span> <span class="toc-text">三、 RNN的挑战与进阶变体</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E4%B8%8E%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%E9%97%AE%E9%A2%98"><span class="toc-number">1.0.9.5.1.</span> <span class="toc-text">1. 梯度消失与梯度爆炸问题</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86%E7%BD%91%E7%BB%9C%EF%BC%88LSTM%EF%BC%89"><span class="toc-number">1.0.9.5.2.</span> <span class="toc-text">2. 长短期记忆网络（LSTM）</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-%E9%97%A8%E6%8E%A7%E5%BE%AA%E7%8E%AF%E5%8D%95%E5%85%83%EF%BC%88GRU%EF%BC%89"><span class="toc-number">1.0.9.5.3.</span> <span class="toc-text">3. 门控循环单元（GRU）</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9B%9B%E3%80%81-RNN%E7%9A%84%E5%85%B8%E5%9E%8B%E5%BA%94%E7%94%A8%E9%A2%86%E5%9F%9F"><span class="toc-number">1.0.9.6.</span> <span class="toc-text">四、 RNN的典型应用领域</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%94%E3%80%81-RNN%E7%9A%84%E7%8E%B0%E7%8A%B6%E4%B8%8E%E6%9C%AA%E6%9D%A5"><span class="toc-number">1.0.9.7.</span> <span class="toc-text">五、 RNN的现状与未来</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-2"><span class="toc-number">1.0.9.8.</span> <span class="toc-text">总结</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="anzhiyufont anzhiyu-icon-history"></i><span>最近发布</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/21/21.%E8%A7%86%E9%A2%91%E7%90%86%E8%A7%A3%E8%AE%BA%E6%96%87%E4%B8%B2%E8%AE%B2/" title="letnet">letnet</a><time datetime="2025-10-21T05:21:33.000Z" title="发表于 2025-10-21 13:21:33">2025-10-21</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/20/20.I3D/" title="letnet">letnet</a><time datetime="2025-10-20T05:21:33.000Z" title="发表于 2025-10-20 13:21:33">2025-10-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/19/19.%E6%96%AF%E5%9D%A6%E7%A6%8F%202022%20%E5%B9%B4%20AI%20%E6%8C%87%E6%95%B0%E6%8A%A5%E5%91%8A%E7%B2%BE%E8%AF%BB/" title="letnet">letnet</a><time datetime="2025-10-19T05:21:33.000Z" title="发表于 2025-10-19 13:21:33">2025-10-19</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/18/18.DeepMind%20AlphaCode/" title="letnet">letnet</a><time datetime="2025-10-18T05:21:33.000Z" title="发表于 2025-10-18 13:21:33">2025-10-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/17/17.OpenAI%20Codex/" title="letnet">letnet</a><time datetime="2025-10-17T05:21:33.000Z" title="发表于 2025-10-17 13:21:33">2025-10-17</time></div></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div id="footer_deal"><a class="deal_link" href="mailto:anzhiyu-c@qq.com" title="email"><i class="anzhiyufont anzhiyu-icon-envelope"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://weibo.com/u/6378063631" title="微博"><i class="anzhiyufont anzhiyu-icon-weibo"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://www.facebook.com/profile.php?id=100092208016287&amp;sk=about" title="facebook"><i class="anzhiyufont anzhiyu-icon-facebook1"></i></a><a class="deal_link" href="/atom.xml" title="RSS"><i class="anzhiyufont anzhiyu-icon-rss"></i></a><img class="footer_mini_logo" title="返回顶部" alt="返回顶部" onclick="anzhiyu.scrollToDest(0, 500)" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://bu.dusays.com/2023/04/27/64496e511b09c.jpg" size="50px"/><a class="deal_link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c" title="Github"><i class="anzhiyufont anzhiyu-icon-github"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://space.bilibili.com/372204786" title="Bilibili"><i class="anzhiyufont anzhiyu-icon-bilibili"></i></a><a class="deal_link" target="_blank" rel="noopener" href="https://v.douyin.com/DwCpMEy/" title="抖音"><i class="anzhiyufont anzhiyu-icon-tiktok"></i></a><a class="deal_link" href="/copyright" title="CC"><i class="anzhiyufont anzhiyu-icon-copyright-line"></i></a></div><div id="anzhiyu-footer"><div class="footer-group"><div class="footer-title">服务</div><div class="footer-links"><a class="footer-item" title="51la统计" target="_blank" rel="noopener" href="https://v6.51.la/">51la统计</a><a class="footer-item" title="十年之约" target="_blank" rel="noopener" href="https://www.foreverblog.cn/">十年之约</a><a class="footer-item" title="开往" target="_blank" rel="noopener" href="https://github.com/travellings-link/travellings">开往</a></div></div><div class="footer-group"><div class="footer-title">主题</div><div class="footer-links"><a class="footer-item" title="文档" href="/docs/">文档</a><a class="footer-item" title="源码" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu">源码</a><a class="footer-item" title="更新日志" href="/update/">更新日志</a></div></div><div class="footer-group"><div class="footer-title">导航</div><div class="footer-links"><a class="footer-item" title="即刻短文" href="/essay/">即刻短文</a><a class="footer-item" title="友链文章" href="/fcircle/">友链文章</a><a class="footer-item" title="留言板" href="/comments/">留言板</a></div></div><div class="footer-group"><div class="footer-title">协议</div><div class="footer-links"><a class="footer-item" title="隐私协议" href="/privacy/">隐私协议</a><a class="footer-item" title="Cookies" href="/cookies/">Cookies</a><a class="footer-item" title="版权协议" href="/copyright/">版权协议</a></div></div><div class="footer-group"><div class="footer-title-group"><div class="footer-title">友链</div><a class="random-friends-btn" id="footer-random-friends-btn" href="javascript:addFriendLinksInFooter();" title="换一批友情链接"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right"></i></a></div><div class="footer-links" id="friend-links-in-footer"></div></div></div></div><div id="footer-bar"><div class="footer-bar-links"><div class="footer-bar-left"><div id="footer-bar-tips"><div class="copyright">&copy;2025 By <a class="footer-bar-link" href="/" title="黎思敏" target="_blank">黎思敏</a></div></div><div id="footer-type-tips"></div></div><div class="footer-bar-right"><a class="footer-bar-link" target="_blank" rel="noopener" href="https://github.com/anzhiyu-c/hexo-theme-anzhiyu" title="主题">主题</a></div></div></div></footer></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="sidebar-site-data site-data is-center"><a href="/archives/" title="archive"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/" title="tag"><div class="headline">标签</div><div class="length-num">0</div></a><a href="/categories/" title="category"><div class="headline">分类</div><div class="length-num">0</div></a></div><span class="sidebar-menu-item-title">功能</span><div class="sidebar-menu-item"><a class="darkmode_switchbutton menu-child" href="javascript:void(0);" title="显示模式"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span>显示模式</span></a></div><div class="back-menu-list-groups"><div class="back-menu-list-group"><div class="back-menu-list-title">网页</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://hexo.anheyu.com/" title="博客"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="/img/favicon.ico" alt="博客"/><span class="back-menu-item-text">博客</span></a></div></div><div class="back-menu-list-group"><div class="back-menu-list-title">项目</div><div class="back-menu-list"><a class="back-menu-item" target="_blank" rel="noopener" href="https://image.anheyu.com/" title="安知鱼图床"><img class="back-menu-item-icon" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null,this.src=&quot;/img/404.jpg&quot;" data-lazy-src="https://image.anheyu.com/favicon.ico" alt="安知鱼图床"/><span class="back-menu-item-text">安知鱼图床</span></a></div></div></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 文章</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><i class="anzhiyufont anzhiyu-icon-box-archive faa-tada" style="font-size: 0.9em;"></i><span> 隧道</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><i class="anzhiyufont anzhiyu-icon-shapes faa-tada" style="font-size: 0.9em;"></i><span> 分类</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags faa-tada" style="font-size: 0.9em;"></i><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 友链</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/link/"><i class="anzhiyufont anzhiyu-icon-link faa-tada" style="font-size: 0.9em;"></i><span> 友人帐</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/fcircle/"><i class="anzhiyufont anzhiyu-icon-artstation faa-tada" style="font-size: 0.9em;"></i><span> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><i class="anzhiyufont anzhiyu-icon-envelope faa-tada" style="font-size: 0.9em;"></i><span> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 我的</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/music/"><i class="anzhiyufont anzhiyu-icon-music faa-tada" style="font-size: 0.9em;"></i><span> 音乐馆</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/bangumis/"><i class="anzhiyufont anzhiyu-icon-bilibili faa-tada" style="font-size: 0.9em;"></i><span> 追番页</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/album/"><i class="anzhiyufont anzhiyu-icon-images faa-tada" style="font-size: 0.9em;"></i><span> 相册集</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/air-conditioner/"><i class="anzhiyufont anzhiyu-icon-fan faa-tada" style="font-size: 0.9em;"></i><span> 小空调</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><span> 关于</span></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/about/"><i class="anzhiyufont anzhiyu-icon-paper-plane faa-tada" style="font-size: 0.9em;"></i><span> 关于本人</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/essay/"><i class="anzhiyufont anzhiyu-icon-lightbulb faa-tada" style="font-size: 0.9em;"></i><span> 闲言碎语</span></a></li><li><a class="site-page child faa-parent animated-hover" href="javascript:toRandomPost()"><i class="anzhiyufont anzhiyu-icon-shoe-prints1 faa-tada" style="font-size: 0.9em;"></i><span> 随便逛逛</span></a></li></ul></div></div><span class="sidebar-menu-item-title">标签</span></div></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="anzhiyufont anzhiyu-icon-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="anzhiyufont anzhiyu-icon-arrows-left-right"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="anzhiyufont anzhiyu-icon-gear"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="anzhiyufont anzhiyu-icon-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></button></div></div><div id="nav-music"><a id="nav-music-hoverTips" onclick="anzhiyu.musicToggle()" accesskey="m">播放音乐</a><div id="console-music-bg"></div><meting-js id="8152976493" server="netease" type="playlist" mutex="true" preload="none" theme="var(--anzhiyu-main)" data-lrctype="0" order="random" volume="0.7"></meting-js></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="anzhiyufont anzhiyu-icon-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="anzhiyufont anzhiyu-icon-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="anzhiyufont anzhiyu-icon-arrow-rotate-right" style="font-size: 1rem;"></i></div><div class="rightMenu-item" id="menu-top"><i class="anzhiyufont anzhiyu-icon-arrow-up"></i></div></div><div class="rightMenu-group rightMenu-line rightMenuPlugin"><div class="rightMenu-item" id="menu-copytext"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制选中文本</span></div><div class="rightMenu-item" id="menu-pastetext"><i class="anzhiyufont anzhiyu-icon-paste"></i><span>粘贴文本</span></div><a class="rightMenu-item" id="menu-commenttext"><i class="anzhiyufont anzhiyu-icon-comment-medical"></i><span>引用到评论</span></a><div class="rightMenu-item" id="menu-newwindow"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开</span></div><div class="rightMenu-item" id="menu-copylink"><i class="anzhiyufont anzhiyu-icon-link"></i><span>复制链接地址</span></div><div class="rightMenu-item" id="menu-copyimg"><i class="anzhiyufont anzhiyu-icon-images"></i><span>复制此图片</span></div><div class="rightMenu-item" id="menu-downloadimg"><i class="anzhiyufont anzhiyu-icon-download"></i><span>下载此图片</span></div><div class="rightMenu-item" id="menu-newwindowimg"><i class="anzhiyufont anzhiyu-icon-window-restore"></i><span>新窗口打开图片</span></div><div class="rightMenu-item" id="menu-search"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>站内搜索</span></div><div class="rightMenu-item" id="menu-searchBaidu"><i class="anzhiyufont anzhiyu-icon-magnifying-glass"></i><span>百度搜索</span></div><div class="rightMenu-item" id="menu-music-toggle"><i class="anzhiyufont anzhiyu-icon-play"></i><span>播放音乐</span></div><div class="rightMenu-item" id="menu-music-back"><i class="anzhiyufont anzhiyu-icon-backward"></i><span>切换到上一首</span></div><div class="rightMenu-item" id="menu-music-forward"><i class="anzhiyufont anzhiyu-icon-forward"></i><span>切换到下一首</span></div><div class="rightMenu-item" id="menu-music-playlist" onclick="window.open(&quot;https://y.qq.com/n/ryqq/playlist/8802438608&quot;, &quot;_blank&quot;);" style="display: none;"><i class="anzhiyufont anzhiyu-icon-radio"></i><span>查看所有歌曲</span></div><div class="rightMenu-item" id="menu-music-copyMusicName"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制歌名</span></div></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" id="menu-randomPost"><i class="anzhiyufont anzhiyu-icon-shuffle"></i><span>随便逛逛</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="anzhiyufont anzhiyu-icon-cube"></i><span>博客分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="anzhiyufont anzhiyu-icon-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item" id="menu-copy" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-copy"></i><span>复制地址</span></a><a class="rightMenu-item" id="menu-commentBarrage" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-message"></i><span class="menu-commentBarrage-text">关闭热评</span></a><a class="rightMenu-item" id="menu-darkmode" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-circle-half-stroke"></i><span class="menu-darkmode-text">深色模式</span></a><a class="rightMenu-item" id="menu-translate" href="javascript:void(0);"><i class="anzhiyufont anzhiyu-icon-language"></i><span>轉為繁體</span></a></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.cbd.int/@fancyapps/ui@5.0.28/dist/fancybox/fancybox.umd.js"></script><script src="https://cdn.cbd.int/instant.page@5.2.0/instantpage.js" type="module"></script><script src="https://cdn.cbd.int/vanilla-lazyload@17.8.5/dist/lazyload.iife.min.js"></script><script src="https://cdn.cbd.int/node-snackbar@0.1.16/dist/snackbar.min.js"></script><canvas id="universe"></canvas><script async src="https://npm.elemecdn.com/anzhiyu-theme-static@1.0.0/dark/dark.js"></script><script>// 消除控制台打印
var HoldLog = console.log;
console.log = function () {};
let now1 = new Date();
queueMicrotask(() => {
  const Log = function () {
    HoldLog.apply(console, arguments);
  }; //在恢复前输出日志
  const grt = new Date("10/21/2025 00:00:00"); //此处修改你的建站时间或者网站上线时间
  now1.setTime(now1.getTime() + 250);
  const days = (now1 - grt) / 1000 / 60 / 60 / 24;
  const dnum = Math.floor(days);
  const ascll = [
    `欢迎使用安知鱼!`,
    `生活明朗, 万物可爱`,
    `
        
       █████╗ ███╗   ██╗███████╗██╗  ██╗██╗██╗   ██╗██╗   ██╗
      ██╔══██╗████╗  ██║╚══███╔╝██║  ██║██║╚██╗ ██╔╝██║   ██║
      ███████║██╔██╗ ██║  ███╔╝ ███████║██║ ╚████╔╝ ██║   ██║
      ██╔══██║██║╚██╗██║ ███╔╝  ██╔══██║██║  ╚██╔╝  ██║   ██║
      ██║  ██║██║ ╚████║███████╗██║  ██║██║   ██║   ╚██████╔╝
      ╚═╝  ╚═╝╚═╝  ╚═══╝╚══════╝╚═╝  ╚═╝╚═╝   ╚═╝    ╚═════╝
        
        `,
    "已上线",
    dnum,
    "天",
    "©2025 By 安知鱼 V1.6.15",
  ];
  const ascll2 = [`NCC2-036`, `调用前置摄像头拍照成功，识别为【小笨蛋】.`, `Photo captured: `, `🤪`];

  setTimeout(
    Log.bind(
      console,
      `\n%c${ascll[0]} %c ${ascll[1]} %c ${ascll[2]} %c${ascll[3]}%c ${ascll[4]}%c ${ascll[5]}\n\n%c ${ascll[6]}\n`,
      "color:#425AEF",
      "",
      "color:#425AEF",
      "color:#425AEF",
      "",
      "color:#425AEF",
      ""
    )
  );
  setTimeout(
    Log.bind(
      console,
      `%c ${ascll2[0]} %c ${ascll2[1]} %c \n${ascll2[2]} %c\n${ascll2[3]}\n`,
      "color:white; background-color:#4fd953",
      "",
      "",
      'background:url("https://npm.elemecdn.com/anzhiyu-blog@1.1.6/img/post/common/tinggge.gif") no-repeat;font-size:450%'
    )
  );

  setTimeout(Log.bind(console, "%c WELCOME %c 你好，小笨蛋.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(
      console,
      "%c ⚡ Powered by 安知鱼 %c 你正在访问 黎思敏 的博客.",
      "color:white; background-color:#f0ad4e",
      ""
    )
  );

  setTimeout(Log.bind(console, "%c W23-12 %c 你已打开控制台.", "color:white; background-color:#4f90d9", ""));

  setTimeout(
    console.warn.bind(console, "%c S013-782 %c 你现在正处于监控中.", "color:white; background-color:#d9534f", "")
  );
});</script><script async src="/anzhiyu/random.js"></script><div class="js-pjax"><input type="hidden" name="page-type" id="page-type" value="post"></div><script>var visitorMail = "";
</script><script async data-pjax src="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/waterfall/waterfall.js"></script><script src="https://cdn.cbd.int/qrcodejs@1.0.0/qrcode.min.js"></script><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.1.9/icon/ali_iconfont_css.css"><link rel="stylesheet" href="https://cdn.cbd.int/anzhiyu-theme-static@1.0.0/aplayer/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.cbd.int/anzhiyu-blog-static@1.0.1/js/APlayer.min.js"></script><script src="https://cdn.cbd.int/hexo-anzhiyu-music@1.0.1/assets/js/Meting2.min.js"></script><script src="https://cdn.cbd.int/pjax@0.2.8/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]
var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {
  // removeEventListener scroll 
  anzhiyu.removeGlobalFnEvent('pjax')
  anzhiyu.removeGlobalFnEvent('themeChange')

  document.getElementById('rightside').classList.remove('rightside-show')
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()
})

document.addEventListener('pjax:error', e => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script charset="UTF-8" src="https://cdn.cbd.int/anzhiyu-theme-static@1.1.5/accesskey/accesskey.js"></script></div><div id="popup-window"><div class="popup-window-title">通知</div><div class="popup-window-divider"></div><div class="popup-window-content"><div class="popup-tip">你好呀</div><div class="popup-link"><i class="anzhiyufont anzhiyu-icon-arrow-circle-right"></i></div></div></div></body></html>