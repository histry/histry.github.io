<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>黎思敏的个人学习成长纪录</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="黎思敏的个人学习成长纪录博客">
<meta property="og:type" content="website">
<meta property="og:title" content="黎思敏的个人学习成长纪录">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="黎思敏的个人学习成长纪录">
<meta property="og:description" content="黎思敏的个人学习成长纪录博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="黎思敏">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="黎思敏的个人学习成长纪录" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">黎思敏的个人学习成长纪录</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Li Simin’s Personal Blog</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS 订阅"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="搜索"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="搜索"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-letnet" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/10/21/letnet/" class="article-date">
  <time class="dt-published" datetime="2025-10-21T05:21:33.000Z" itemprop="datePublished">2025-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/10/21/letnet/">letnet</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="1-LeNet-1998"><a href="#1-LeNet-1998" class="headerlink" title="1.LeNet (1998)"></a>1.LeNet (1998)</h1><p><a href="LeNet.pdf">LeNet.pdf</a> </p>
<p><strong>论文简读：</strong><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/273247515">(91 封私信) Lenet5经典论文解读 - 知乎</a></p>
<p>这篇论文由Yann LeCun等人发表，是深度学习发展史上至关重要的里程碑，它首次成功展示了如何通过<strong>反向传播</strong>训练一个<strong>端到端</strong>的卷积神经网络来解决真实的图像识别任务。</p>
<h2 id="论文核心信息"><a href="#论文核心信息" class="headerlink" title="论文核心信息"></a>论文核心信息</h2><ul>
<li><strong>标题</strong>: <strong>Gradient-Based Learning Applied to Document Recognition</strong></li>
<li><strong>作者</strong>: Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner</li>
<li><strong>发表期刊&#x2F;年份</strong>: Proceedings of the IEEE, 1998</li>
<li><strong>核心贡献</strong>: 提出了<strong>LeNet-5</strong> 架构，系统性地阐述了<strong>卷积神经网络</strong> 的基本组件，并成功将其应用于手写数字识别，推动了基于数据的机器学习范式。</li>
</ul>
<hr>
<h2 id="一、背景与动机：LeNet之前的世界"><a href="#一、背景与动机：LeNet之前的世界" class="headerlink" title="一、背景与动机：LeNet之前的世界"></a>一、背景与动机：LeNet之前的世界</h2><p>在LeNet之前，模式识别和文档识别领域主要依赖于：</p>
<ol>
<li><strong>手工设计的特征</strong>：专家需要设计特定的特征提取器（如边缘、角点检测器）。</li>
<li><strong>传统的分类器</strong>：如线性分类器、K近邻等。</li>
</ol>
<p>这种**“特征工程 + 分类器”** 的流程非常繁琐，且严重依赖领域知识。LeCun等人的目标是开发一个能够<strong>直接从像素数据中自动学习特征</strong>的系统。</p>
<hr>
<h2 id="二、LeNet-5-架构详解"><a href="#二、LeNet-5-架构详解" class="headerlink" title="二、LeNet-5 架构详解"></a>二、LeNet-5 架构详解</h2><p>LeNet-5是论文中介绍的用于手写数字识别的网络，其名称中的“5”表示它具有5层带参数的层（2层卷积+2层下采样+1层全连接，输出层不计入）。下图清晰地展示了其数据流动和层次结构：</p>
<p><img src="/attachments/image-20251013122923111.png" alt="image-20251013122923111"></p>
<figure class="highlight plaintext"><figcaption><span>TD</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">flowchart TD</span><br><span class="line">    A[&quot;输入图像&lt;br&gt;32x32&quot;] --&gt; B[&quot;C1: 卷积层&lt;br&gt;6@28x28&quot;]</span><br><span class="line">    B --&gt; C[&quot;S2: 池化层&lt;br&gt;6@14x14&quot;]</span><br><span class="line">    C --&gt; D[&quot;C3: 卷积层&lt;br&gt;16@10x10&quot;]</span><br><span class="line">    D --&gt; E[&quot;S4: 池化层&lt;br&gt;16@5x5&quot;]</span><br><span class="line">    E --&gt; F[&quot;C5: 卷积/全连接层&lt;br&gt;120@1x1&quot;]</span><br><span class="line">    F --&gt; G[&quot;F6: 全连接层&lt;br&gt;84&quot;]</span><br><span class="line">    G --&gt; H[&quot;输出层&lt;br&gt;10 (数字0-9)&quot;]</span><br></pre></td></tr></table></figure>

<p>下面我们逐层解析这个架构：</p>
<h3 id="1-输入层"><a href="#1-输入层" class="headerlink" title="1. 输入层"></a>1. 输入层</h3><ul>
<li><strong>输入尺寸</strong>: <code>32 x 32</code> 像素的灰度图像。</li>
<li><strong>为什么是32x32？</strong> 原始的手写数字数据库（MNIST的前身）图像尺寸为 <code>28x28</code>。填充到 <code>32x32</code> 是为了让卷积后在中心区域得到 <code>28x28</code> 的特征图，便于后续处理。</li>
</ul>
<h3 id="2-C1层-卷积层"><a href="#2-C1层-卷积层" class="headerlink" title="2. C1层 - 卷积层"></a>2. C1层 - 卷积层</h3><ul>
<li><strong>操作</strong>: 使用 <code>6</code> 个 <code>5x5</code> 的卷积核，步长为 <code>1</code>。</li>
<li><strong>输出</strong>: <code>6</code> 个 <code>28x28</code> 的特征图 <code>(32-5+1=28)</code>。</li>
<li><strong>作用</strong>: 学习基础的局部特征，如边缘、角点等。</li>
</ul>
<h3 id="3-S2层-池化层"><a href="#3-S2层-池化层" class="headerlink" title="3. S2层 - 池化层"></a>3. S2层 - 池化层</h3><ul>
<li><strong>操作</strong>: <strong>平均池化</strong>，窗口大小 <code>2x2</code>，步长为 <code>2</code>。</li>
<li><strong>输出</strong>: <code>6</code> 个 <code>14x14</code> 的特征图 <code>(28/2=14)</code>。</li>
<li><strong>作用</strong>:<ul>
<li><strong>降维</strong>，减少计算量和参数。</li>
<li><strong>提供平移不变性</strong>，使网络对特征位置的微小变化不敏感。</li>
<li><strong>扩大感受野</strong>，使后续层能看到更广的输入区域。</li>
</ul>
</li>
</ul>
<h3 id="4-C3层-卷积层"><a href="#4-C3层-卷积层" class="headerlink" title="4. C3层 - 卷积层"></a>4. C3层 - 卷积层</h3><ul>
<li><strong>操作</strong>: 使用 <code>16</code> 个 <code>5x5</code> 的卷积核。</li>
<li><strong>输出</strong>: <code>16</code> 个 <code>10x10</code> 的特征图 <code>(14-5+1=10)</code>。</li>
<li><strong>关键细节</strong>: 这一层采用了<strong>不完全连接</strong>的设计。并不是S2层的每个特征图都连接到C3层的所有特征图。这样做是为了<strong>打破对称性</strong>，强制不同的特征图学习互补的特征，同时减少参数数量。连接关系由一个预设的表决定。</li>
</ul>
<h3 id="5-S4层-池化层"><a href="#5-S4层-池化层" class="headerlink" title="5. S4层 - 池化层"></a>5. S4层 - 池化层</h3><ul>
<li><strong>操作</strong>: 同样是 <code>2x2</code> 的平均池化，步长为 <code>2</code>。</li>
<li><strong>输出</strong>: <code>16</code> 个 <code>5x5</code> 的特征图 <code>(10/2=5)</code>。</li>
</ul>
<h3 id="6-C5层-卷积-全连接层"><a href="#6-C5层-卷积-全连接层" class="headerlink" title="6. C5层 - 卷积&#x2F;全连接层"></a>6. C5层 - 卷积&#x2F;全连接层</h3><ul>
<li><strong>操作</strong>: 使用 <code>120</code> 个 <code>5x5</code> 的卷积核。</li>
<li><strong>输出</strong>: <code>120</code> 个 <code>1x1</code> 的特征图 <code>(5-5+1=1)</code>。</li>
<li><strong>解读</strong>: 由于输入是 <code>5x5</code>，卷积核也是 <code>5x5</code>，所以输出的每个点都看到了S4层对应特征图的全部区域。因此，从功能上看，<strong>C5层等价于一个全连接层</strong>。</li>
</ul>
<h3 id="7-F6层-全连接层"><a href="#7-F6层-全连接层" class="headerlink" title="7. F6层 - 全连接层"></a>7. F6层 - 全连接层</h3><ul>
<li><strong>操作</strong>: 全连接，包含 <code>84</code> 个神经元。</li>
<li><strong>激活函数</strong>: 使用 <code>tanh</code> 或 <code>sigmoid</code>。</li>
<li><strong>为什么是84？</strong> 一种解释是，输出层的设计便于编码10个数字的7段数码管显示，而7x12的比特图表示一个字符需要84个像素。</li>
</ul>
<h3 id="8-输出层"><a href="#8-输出层" class="headerlink" title="8. 输出层"></a>8. 输出层</h3><ul>
<li><strong>操作</strong>: 全连接，包含 <code>10</code> 个神经元，分别对应数字 <code>0-9</code>。</li>
<li><strong>激活函数</strong>: <strong>径向基函数</strong>，或者可以理解为 <strong>欧几里得距离</strong>。每个输出神经元计算输入向量（来自F6）与该类别原型的距离，距离越小，输出越大，表示属于该类别的概率越高。现代网络通常直接用 <strong>Softmax</strong> 代替。</li>
</ul>
<hr>
<h2 id="三、关键技术与创新点"><a href="#三、关键技术与创新点" class="headerlink" title="三、关键技术与创新点"></a>三、关键技术与创新点</h2><p>LeNet-5虽然结构相对简单，但它几乎包含了现代CNN的所有核心要素：</p>
<ol>
<li><strong>卷积层</strong>： 通过<strong>局部连接</strong>和<strong>权值共享</strong>，极大地减少了网络参数，使网络能够高效地学习空间层次特征。</li>
<li><strong>池化层</strong>： 引入下采样，增强了模型的鲁棒性。</li>
<li><strong>端到端训练</strong>： 整个系统（从特征提取到分类）使用<strong>梯度下降法</strong>和<strong>反向传播</strong>进行联合训练，无需单独训练各个模块。</li>
<li><strong>从原始像素中学习</strong>： 证明了机器可以自动从数据中学习有意义的特征表示，减少了对手工特征工程的依赖。</li>
<li><strong>成功应用于实际问题</strong>： 在邮政信件的邮政编码识别系统中取得了巨大成功，展示了其商业价值。</li>
</ol>
<hr>
<h2 id="四、历史意义与影响"><a href="#四、历史意义与影响" class="headerlink" title="四、历史意义与影响"></a>四、历史意义与影响</h2><ol>
<li><strong>奠定了CNN的基础</strong>： LeNet-5是第一个真正意义上的卷积神经网络，其“卷积-池化-全连接”的结构范式被后续几乎所有深度学习模型所沿用。</li>
<li><strong>启发了深度学习复兴</strong>： 虽然它在当时没有引起足够的轰动（受限于数据和算力），但其思想直接启发了2012年引爆深度学习革命的<strong>AlexNet</strong>。AlexNet可以看作是一个“更深、更宽、用了ReLU和Dropout的LeNet”。</li>
<li><strong>推动了相关数据集的发展</strong>： 论文中使用的MNIST数据集（一个更大的手写数字库）后来成为了机器学习入门和基准测试的“Hello World”。</li>
</ol>
<hr>
<h2 id="五、与现代CNN的差异"><a href="#五、与现代CNN的差异" class="headerlink" title="五、与现代CNN的差异"></a>五、与现代CNN的差异</h2><p>以今天的眼光看，LeNet-5有一些“古老”的设计：</p>
<ul>
<li><p><strong>激活函数</strong>： 使用 <code>tanh</code> 或 <code>sigmoid</code>，而非现在更流行的 <strong>ReLU</strong>。</p>
</li>
<li><p><strong>池化方式</strong>： 使用 <strong>平均池化</strong>，而现在更常用 <strong>最大池化</strong>。</p>
</li>
<li><p><strong>参数初始化</strong>： 需要一些特殊的技巧，而现代有Xavier&#x2F;Glorot等更标准的方法。</p>
</li>
<li><p><strong>正则化</strong>： 没有使用 <strong>Dropout</strong>、<strong>批归一化</strong> 等现代正则化技术。</p>
</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>LeNet论文的伟大之处在于，它<strong>首次完整地提出并实践了卷积神经网络的蓝图</strong>。它不仅仅是一个模型，更是一套<strong>方法论</strong>，证明了通过梯度下降法，一个具有局部连接、权值共享和空间子采样结构的神经网络，能够直接从像素中学习到强大的视觉特征。</p>
<p>它像一颗种子，虽然在其诞生后的近十年里处于“休眠”状态，但一旦遇到大数据（如ImageNet）和强算力（如GPU），便迅速生长为参天大树，开启了整个人工智能的新时代。理解LeNet，是理解所有现代深度卷积神经网络的基础。</p>
<p>LeNet-5是卷积神经网络（CNN）的开山之作，由Yann LeCun等人于1998年提出，最初用于解决手写数字识别问题。为了帮助你快速把握其全貌，我先用一个表格汇总其核心信息，再展开说明细节：</p>
<table>
<thead>
<tr>
<th align="left">维度</th>
<th align="left">核心内容</th>
</tr>
</thead>
<tbody><tr>
<td align="left"><strong>论文标题</strong></td>
<td align="left"><em>Gradient-Based Learning Applied to Document Recognition</em></td>
</tr>
<tr>
<td align="left"><strong>作者与时间</strong></td>
<td align="left">Yann LeCun等 (1998年)</td>
</tr>
<tr>
<td align="left"><strong>核心贡献</strong></td>
<td align="left">1. <strong>首次成功应用的CNN之一</strong>：证明了CNN在真实世界问题中的有效性。 2. <strong>奠定CNN基本结构</strong>：确立了“卷积层-池化层-全连接层”的经典范式。 3. <strong>端到端学习范式</strong></td>
</tr>
<tr>
<td align="left"><strong>技术创新点</strong></td>
<td align="left">1. <strong>局部连接与权值共享</strong>：大幅减少参数数量。 2. <strong>空间降采样（池化）</strong>：增强平移不变性，降低计算复杂度。 3. <strong>多层特征提取</strong>：通过交替卷积与池化，自动学习层次化特征。</td>
</tr>
<tr>
<td align="left"><strong>实验结果</strong></td>
<td align="left">在美国邮政手写数字识别任务上取得<strong>超过99%的准确率</strong>，并成功投入实际应用。</td>
</tr>
</tbody></table>
<h3 id="🔬-技术创新点"><a href="#🔬-技术创新点" class="headerlink" title="🔬 技术创新点"></a>🔬 技术创新点</h3><ol>
<li><strong>局部连接与权值共享</strong>：LeNet-5的卷积层中，每个神经元只与前一层的一个局部区域连接（如5x5的卷积核），并且同一个特征图共享同一组权重（卷积核）。这显著减少了参数数量（例如，C1层仅有156个参数），降低了过拟合风险，并让网络能够专注于提取图像的<strong>局部特征</strong>。</li>
<li><strong>空间降采样（池化层）</strong>：LeNet-5使用<strong>平均池化</strong>（当时论文中描述为下采样层），对特征图进行降维。例如，S2池化层使用2x2的池化单元，将28x28的输入降采样为14x14的输出。这不仅降低了计算复杂度，也赋予了网络一定的<strong>平移不变性</strong>。</li>
<li><strong>层次化特征提取结构</strong>：LeNet-5通过<strong>交替堆叠卷积层和池化层</strong>，自动从原始像素中学习由低到高、由局部到全局的特征。早期的卷积层捕捉边缘等基础特征，后续层则组合这些基础特征形成更复杂的模式（如数字的形状）。最后通过全连接层（F6层和输出层）进行分类。</li>
</ol>
<h3 id="📊-实验结果"><a href="#📊-实验结果" class="headerlink" title="📊 实验结果"></a>📊 实验结果</h3><p>LeNet-5在<strong>手写数字识别</strong>任务上取得了巨大成功，其识别准确率<strong>超过99%</strong>。这一性能在当时远超其他方法，也使其成功应用于<strong>美国邮政系统的邮政编码识别</strong>和<strong>银行行业的支票处理</strong>，展现了其实际应用价值。</p>
<h3 id="🤔-个人思考"><a href="#🤔-个人思考" class="headerlink" title="🤔 个人思考"></a>🤔 个人思考</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ul>
<li><strong>开创性贡献</strong>：作为早期成功的CNN之一，LeNet-5为深度学习在计算机视觉领域的发展奠定了基础。</li>
<li><strong>结构简单高效</strong>：网络结构清晰，参数数量相对较少，在一些资源受限的场景或简单分类任务上仍有一定价值。</li>
<li><strong>端到端学习</strong>：避免了传统方法中复杂的手工特征设计，实现了从原始数据到最终输出的自动学习。</li>
</ul>
<h4 id="不足"><a href="#不足" class="headerlink" title="不足"></a>不足</h4><ul>
<li><strong>处理大规模数据能力较弱</strong>：受限于当时的硬件条件和数据规模，LeNet-5难以应对像ImageNet那样的大型数据集。</li>
<li><strong>网络结构较浅</strong>：仅有7层（包含输入输出层），<strong>特征提取和抽象能力有限</strong>，难以捕捉非常复杂的模式。</li>
<li><strong>激活函数和正则化技术相对传统</strong>：LeNet-5使用了<strong>tanh或sigmoid</strong>作为激活函数，这些函数在更深层的网络中容易导致梯度消失问题。同时，它也没有采用像Dropout这样现代的正则化技术。</li>
</ul>
<h4 id="启发"><a href="#启发" class="headerlink" title="启发"></a>启发</h4><ul>
<li>LeNet-5的成功验证了<strong>局部连接、权值共享和空间降采样</strong>这些核心思想的强大之处，这些思想至今仍是许多深度神经网络模型的基础。</li>
<li>它展示了如何通过<strong>多层网络结构来自动学习层次化的特征表示</strong>，这一理念被后续许多深度学习模型所继承和发展。</li>
<li>LeNet-5的实践表明了<strong>理论与实际应用结合</strong>的重要性，一个好的模型不仅要在理论上有效，更要能解决实际问题。</li>
</ul>
<h3 id="📚-相关论文"><a href="#📚-相关论文" class="headerlink" title="📚 相关论文"></a>📚 相关论文</h3><h4 id="前序工作"><a href="#前序工作" class="headerlink" title="前序工作"></a>前序工作</h4><p>在LeNet-5之前，<strong>神经认知机</strong> 被认为是卷积神经网络的早期雏形。LeNet系列本身也经历了多次迭代（如LeNet-1到LeNet-4）才发展到LeNet-5。</p>
<h4 id="后续发展"><a href="#后续发展" class="headerlink" title="后续发展"></a>后续发展</h4><p>LeNet-5之后，卷积神经网络经历了快速的发展，涌现出许多重要的模型：</p>
<ul>
<li><strong>AlexNet (2012年)</strong>：在2012年ImageNet竞赛中夺冠，真正推动了深度学习的”井喷式”大发展。它采用了<strong>ReLU激活函数、Dropout和数据增强</strong>等技术。</li>
<li><strong>VGGNet (2014年)</strong>：探索了网络的深度，通过堆叠多个<strong>3x3的小卷积核</strong>来构建更深的网络。</li>
<li><strong>ResNet (2015年)</strong>：引入了<strong>残差连接</strong>结构，有效解决了深层网络的梯度消失问题，使得训练上百层的网络成为可能。</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/10/21/letnet/" data-id="cuidK0J01Zudmisu8VZ-9uKF9" data-title="letnet" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/10/21/hello-world/" class="article-date">
  <time class="dt-published" datetime="2025-10-21T03:24:21.176Z" itemprop="datePublished">2025-10-21</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/10/21/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/10/21/hello-world/" data-id="cuidmV0L-5aNNgTm1DesHBH9E" data-title="Hello World" class="article-share-link"><span class="fa fa-share">分享</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/10/">十月 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/10/21/letnet/">letnet</a>
          </li>
        
          <li>
            <a href="/2025/10/21/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 黎思敏<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>